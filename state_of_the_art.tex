\chapter{State of the Art}
\label{cha:soa}
\vspace{0.4 cm}

In this chapter, the current state of the art is analyzed in the context of electricity data representation and time series forecasting methods.
In the first section, a brief introduction to the proposed standards for electricity data representation is presented.
Subsequently, various technologies presented in the literature for time series forecasting are discussed.
In particular, several implementations and use cases are presented.
Additionally, an extensive analysis is conducted on two prominent subjects in the research community: Transformers and Automated Machine Learning (AutoML).
These topics are thoroughly explored in dedicated subsections.
Furthermore, the three use cases of interest, electricity demand forecasting, consumption baseline forecasting, and electricity production forecasting, are treated in more detail in dedicated sections.
At the end of this chapter, it will be clear the context around which the proposed system is developed.


\section{Electricity data representation}
\label{sec:data}
\vspace{0.2 cm}

In this section, a discussion of electricity data representation and proposed standards is presented.
Data issues and frameworks for handling this kind of data are also discussed.

One of the most popular standards for energy data is the Green Button Data\footnote{ \url{https://www.greenbuttondata.org/} }, which is an industry initiative in response to the 2012 White House call-to-action to provide customers easy and secure access to their energy usage information in a both consumer-friendly and computer-friendly format.
The data supported is not limited to electricity but may also include natural gas, and water usage.
Customers using this service are able to securely and easily download their own detailed energy usage in a standard format.
This possibility allows customers to choose to upload their own energy data to a third-party application or automate the secure transfer to authorized third parties, based on affirmative customer consent and control.
It is a very powerful initiative in the U.S. that enables a variety of new services.
Services like UtilityAPI are compatible with the Green Button standard providing support for APIs and XML schemas\footnote{ \url{https://utilityapi.com/docs/greenbutton} }.

\cite{Nguyen2019} provided a case study and explained the lessons learned through the roll-out of Green Button electricity, natural gas, and water data-access initiative, in order to make readily available energy and water consumption data for consumers and third-party companies, that can assist customers while ensuring security and privacy of their data.
This paper presented a case study using the Green Button standard and the steps taken to ensure data security and privacy while enabling access to those consumption data by the consumer and third parties.
Data security and privacy were achieved through the use of the Green Button standard and subsequent implementation by the Green Button Alliance of a compliance-testing program.
Considerations and solutions were needed for data in transit, data at rest, and the authorization mechanisms for allowing unregulated third-party companies to interface directly with utilities on behalf of the consumer while ensuring the consumer maintains complete control of what is to be shared and the ability to revoke that sharing at any time.

In \cite{CHEN201798},  Chen et al. studied the quality of electricity consumption data in a smart grid environment.
They defined and classified which are common data quality issues that may arise in this field.
In particular, the data quality issues related to electricity consumption are classified into three types: noisy data, incomplete data, and outlier data.
These three types of data quality issues are discussed.
The paper introduced the causes of electricity consumption outlier data and provided a review of the possible detection methods.
This is a relevant study since most industrial studies in this field use real-world data that presents these issues.

\cite{8577770} presented a big-data-based framework for dealing with electricity consumption behavior.
It conducted an analysis of the current state-of-the-art methodologies for the extraction of electro-information and it presented the drawbacks of existing modeling strategies for power consumption behavior.
The paper proposed a way of integrating multiple-dimensional information into electricity consumption data, such as weather, holiday, and economic level.
It dealt also with issues such as anonymization, abnormal detections, and meter failures.
The final objective was to conduct an in-depth study for pattern identification, relational analysis, and understanding of the possible actions to perform based on electricity usage.


\section{Time series forecasting}
\label{sec:timeseries}
\vspace{0.2 cm}

In this section, a brief introduction to forecasting competitions, the use of cross-validation for the evaluation of time series forecasting methods, and techniques for time series forecasting is presented.
Time series forecasting is a classic and well-studied topic.
The classical approaches, which are based on statistical methods such as Auto-Regressive Integrated Moving Average (ARIMA), have been joined in recent years by standard Machine Learning (ML) methods and recently also by Deep Learning (DL) methods with the use of neural networks and transformers.

A brief review of forecasting competitions was presented in \cite{HYNDMAN20207}.
These competitions were proposed to promote the development of new solutions and novel techniques in this field.
Over time, these competitions have gained significant attention, resulting in a growing interest in time series forecasting and ultimately fostering the development of fascinating and impactful solutions.
The first and most influential forecasting competitions were, and currently are, the M-competitions\footnote{ \url{https://en.wikipedia.org/wiki/Makridakis_Competitions} }.
These competitions promoted the application of the most recent statistical approaches such as ARIMA, ML approaches such as Support Vector Regression (SVR), and DL approaches such as Long Short-Term Memory (LSTM) networks developed over time to be applied to the forecasting field.
Issues like the statistical significance of the results, cheating using part of the test sets for training, and reproducibility of the results were addressed over the competitions.
Also, other competitions were held such as Sante Fe competitions, the KDD cup, Neural network competitions, Kaggle time series competitions, and Global energy forecasting competitions.

\cite{SPILIOTIS202037} presented an interesting discussion on whether forecasting competition data are representative of reality.
This is a very important point since the performance of new forecasting methods is typically evaluated by relying on data from past forecasting competitions.
However, due to their many limitations, these datasets might not be indicative.
Since obtaining a complete picture of the real world is impossible in practice, the paper proposed to use the M4 competition data as an indication of the real world.
This is reasonable since this data set is composed of many series from the business world.
The properties of this dataset were compared with past datasets, showing that many popular benchmarks may deviate from reality.
The main differences observed were referred to the abnormality of the data, in fact, data from the real world presents relatively more skewed series with outliers, as well as their limited randomness\slash trend.

In \cite{BERGMEIR2012192}, Bergmeir and Benítez presented a study on the use of cross-validation for time series forecasting methods evaluation.
They aimed to combine the evaluation of traditional forecasting procedures, on the one hand, and the evaluation of ML techniques on the other hand.
In fact, in traditional forecasting, a part from the end of each time series is reserved for testing, and the rest is used for training.
Instead, when evaluating ML and other regression methods, often cross-validation is used in the evaluation process without paying much attention to the fact that there are theoretical problems concerning temporal evolutionary effects and dependencies within the data that invalidate the fundamental assumptions of cross-validation, such as to have i.i.d. (independent and identically distributed) data.
They suggested that the use of a blocked form of cross-validation for time series evaluation should be the standard procedure, thus using all available information and circumventing the theoretical problems.
They also affirmed that the use of cross-validation techniques, together with adequate control for stationarity, led to a more robust model selection.

In \cite{Cerqueira2020}, Cerqueira et al. studied the application of performance estimation methods to time series forecasting.
They stated that the dependency among observations in time series raises some caveats about the most appropriate way to estimate models’ performance since cross-validation cannot be applied to this type of data.
Results of a comparative study of different performance estimation methods showed noticeable differences among them.
In particular, their empirical experiments suggested that blocked cross-validation can be applied to stationary time series.
However, when the time series are non-stationary, the most accurate performance estimation methods were out-of-sample methods and in particular the holdout approach repeated in multiple testing periods showed the most accurate estimates.

Subsequently, relevant studies within the context of time series forecasting are outlined, beginning with classical approaches and culminating in contemporary state-of-the-art approaches.

\cite{DEGOOIJER2006443} is a paper published in 2006 and it reviewed the research into time series forecasting made from 1982 to 2005.
Many relevant methods were presented such as exponential smoothing, ARIMA, state space models, structural models, Kalman filter, regime-switching models, functional-coefficient models, neural networks, and many others also involving the combination of approaches.
There were also a lot of relevant studies that presented theoretical concepts such as Seasonality, Forecast evaluation and accuracy measures, and Prediction intervals and densities.
The authors concluded by saying that enormous progress has been made in many areas, but that there were a large number of topics that need further development such as multivariate time series forecasting, forecasting methods based on nonlinear models, model selection procedures, robust statistical methods, and improved forecast intervals.

In \cite{Nesreen2010}, Nesreen et al. presented a large-scale comparison study for the major ML models adopted for time series forecasting.
The models considered were Multi-Layer Perceptron (MLP), Bayesian neural networks, radial basis functions, Generalized Regression Neural Networks (GRNN), K-Nearest Neighbor (KNN) regression, Classification and Regression Trees (CART), SVR, and Gaussian processes.
The study revealed significant differences between the different methods and proclaimed as the best two methods on the monthly M3 time series competition data the MLP and the Gaussian process regression.

\cite{BENTAIEB20127067} proposed a review and a comparison of different strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition.
They also considered the effects of deseasonalization, input variable selection, and combination on the strategies.
From experimental results, they figured out that:
(i) Multiple-Output strategies were the best-performing approaches,
(ii) deseasonalization led to uniformly improved forecast accuracy,
and (iii) input selection was more effective when performed in conjunction with deseasonalization.

In \cite{TEALAB2018334}, Tealab studied the advances in time series forecasting models using Artificial Neural Network (ANN) methodologies.
He took into consideration the papers published from 2006 to 2016.
From the analysis of the papers, he concluded that, although many studies presented the application of neural network models, few of them proposed new neural network models for forecasting.
He found out that these many studies had a similar hybrid methodology that consisted in adjusting a linear time series model, and subsequently using the residuals as the input variables of an ANN model.

\cite{Athiyarath2020} proposed a comparative study and analysis of various time series forecasting techniques such as linear regression model, ARIMA, LSTM, and many others.
In particular, it explored their limitations and utility for different types of time series data across different domains.

\cite{SEZER2020106181} provided a comprehensive literature review of DL studies with a focus on financial time series forecasting implementation.
They categorized the studies according to their intended forecasting implementation areas and grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and LSTM.

In \cite{HEWAMALAGE2021388}, Hewamalage et al. presented an empirical study resulting in an open-source software framework for time series forecasting using Recurrent Neural Network (RNN) architectures.
They concluded that RNN architectures were capable of directly modeling seasonality when the time series presented homogeneous seasonal patterns.
If this is not the case, they recommend a deseasonalization step for achieving better results.
They also provided some comparisons against exponential smoothing and ARIMA demonstrating that RNN models even were not perfect, they were good alternatives.

In \cite{Lim2021}, Lim and Zohren analyzed the main architecture used in both one-step-ahead and multi-horizon time series forecasting.
They described how temporal information is incorporated into predictions by each of the models, such as CNNs, RNNs, and networks with attention mechanisms.
They also highlighted the recent developments in hybrid DL models, which combine statistical models with neural network components to improve performance.

\cite{Masini2023} presented the recent ML advances for time series forecasting.
They started by analyzing the linear methods, paying more attention to penalized regressions and ensembles of models.
They continued presenting nonlinear methods, including tree-based methods, such as Random Forest and Gradient-Boosted Decision Trees (GBDT), and shallow and deep neural networks, in their feed-forward and recurrent versions.
Finally, they also consider ensemble and hybrid models by combining different alternatives.

These reviews have aided in understanding the most commonly utilized methods in time series forecasting and identifying potentially effective solutions for general use cases.
Furthermore, specific studies that present distinct methodologies are carefully examined and analyzed.

\cite{ZHANG2003159} presented a hybrid model combining ARIMA and ANN models.
This was an innovative study of 2003 and was done to take advantage of both strengths of the ARIMA and the ANN models in linear and nonlinear modeling respectively.
The experimental results indicated that the combination was an effective way to improve the forecasting accuracy achieved by both models used separately.

\cite{CAO2003321} proposed to use an approach including multiple Support Vector Machines (SVMs) for time series forecasting.
The multiple SVMs that best fit the regions partitioned by a self-organizing feature map were constructed by finding the most appropriate kernel function and the optimal parameters of the different SVMs.
Simulation results showed that the proposed multiple SVMs approach achieved significant improvement in the generalization performance in comparison with the single SVMs models.
In addition, it was shown that the multiple SVMs were also able to converge faster and use fewer support vectors.

\cite{6210391} attempted to develop an automatic ANN modeling scheme for time series forecasting.
This scheme was based on the GRNN, a special type of neural network.
By taking advantage of several GRNN properties (i.e., a single design parameter and fast learning) and by incorporating several design strategies (e.g., fusing multiple GRNNs), they were able to make the proposed modeling scheme to be effective for modeling large-scale business time series.

\cite{Oliveira2015} described a new type of ensemble improving the predictive performance with respect to existing ensembles for time series forecasting.
In particular, they proposed a new form of diversity generation that explores some specific properties of time series prediction tasks.
Their experiments confirmed that the proposed method for generating diversity was able to improve the performance of the equivalent ensembles with standard diversity generation procedures.

In the study conducted by Sean and Letham \cite{Sean2017}, they introduced Prophet, a modular regression model that offers interpretable parameters.
One of the notable advantages of Prophet is its flexibility in parameter adjustment based on domain knowledge, allowing for intuitive customization based on the characteristics of the time series being analyzed.
Additionally, the researchers also outlined a crucial component that measures and tracks forecast accuracy.
This feature serves to flag forecasts that should be manually reviewed, allowing incremental improvements in the forecasting process.
This capability is vital as it helps in identifying when adjustments are required for the existing model or when an entirely different model may be more suitable for accurate predictions.

In \cite{Borovykh2017}, Borovykh et al. presented a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture.
The proposed method took advantage of dilated convolutions for considering a broad history horizon when forecasting.
The conditioning process in the study involved the application of multiple parallel convolutional filters to individual time series.
This approach facilitated efficient data processing and allowed the exploitation of the correlation structures present among the multivariate time series.
Through experimental evaluations, it was demonstrated that the proposed network was particularly suitable for regression-type problems.
Notably, the network exhibited the capability to effectively learn dependencies within and between the time series, even in cases where extensive historical time series data was not available.
Moreover, the performance of the network surpassed that of linear and recurrent models, highlighting its superior predictive capabilities.

\cite{DEOSANTOSJUNIOR201972} proposed a hybrid system that searches for a suitable function to combine the forecasts of linear and nonlinear models.
The proposed system performed linear and nonlinear modeling of the time series and a data-driven combination that searches for the most suitable function, between linear and nonlinear formalisms, and also the number of models that maximizes the performance of the combination.
As a linear model, the ARIMA model is used and as nonlinear models, MLP and SVR were used.
Experimental results showed that the proposed hybrid system attains superior performance when compared to both single and hybrid models previously reported in the literature.

In \cite{SHEN2020302}, Shen et al. proposed SeriesNet, which is a novel time series forecasting model able to learn features of time series data in different interval lengths.
It is composed of a LSTM network and a dilated causal convolution network.
The fact that the proposed model could learn multi-range and multi-level features from time series data led to a higher predictive accuracy compared to those models using fixed time intervals.

\cite{SMYL202075} presented the winning submission of the M4 forecasting competition.
The winning approach employed a dynamic computational graph neural network system, which combined a standard exponential smoothing model with advanced LSTM networks within a unified framework.
This novel hybrid and hierarchical forecasting method demonstrated superior performance compared to all other models submitted in the competition.
The successful integration of these techniques resulted in enhanced forecasting capabilities, showcasing the effectiveness of the proposed approach in surpassing existing methodologies.

In the context of building better pricing modeling and forecasting frameworks to meet difficulties, \cite{en16031371} proposed to combine seasonal and trend decomposition utilizing LOESS (locally estimated scatterplot smoothing) and Prophet methodologies to perform a more accurate and resilient time series analysis of Italian electricity spot prices.
The proposed method could assist in enhancing projections and providing a better understanding of the variables driving the data.
Experimental results showed that the combination of approaches improved the forecast accuracy and lowered the Mean Absolute Percentage Error (MAPE) performance metric by 18\% compared to the Prophet baseline model.


\vspace{0.1 cm}
\subsection{Attention and transformers}
\label{sec:transformers}
\vspace{0.1 cm}

In this subsection, an overview of attention-based and transformer approaches is presented, with a focus on time series forecasting applications.

\cite{NIU202148} provided an overview of the state-of-the-art attention models proposed.
They classified existing attention models according to the following criteria: the softness of attention, forms of input features, input representation, and output representation.
They also summarized the network architectures used in conjunction with the attention mechanism and described some applications where attention mechanisms are used to improve performance.
Finally, they discussed the interpretability that attention mechanisms provide to the DL models, but also the challenges and prospects of attention models.

\cite{9586824} presented a review of the developed attention mechanism with a focus on the neural machine translation task.
They covered the most adopted attention models and their variants, such as self-attention, soft attention, hard attention, local attention, global attention, additive attention, and multiplicative attention.

\cite{9892274} analyzed that attention mechanisms have raised significant interest in the research community since they promised relevant improvements in the performance of neural network architectures.
In particular, since self-attention was proposed, it has been widely used in transformer-like architectures and has led to significant breakthroughs in many applications.
In the work, Pedro and Oliveira performed an objective comparison of several different attention mechanisms for the classification of samples in the Skin Cancer MNIST dataset.
The results showed that attention modules only sometimes improved the performance of CNN architectures, but also that this improvement was not consistent in different settings.
On the other hand, the results obtained with self-attention mechanisms showed consistent and significant improvements, leading to the best results even in architectures with a reduced number of parameters.

The study presented in \cite{LI2019104785} introduced an evolutionary attention learning approach for enhancing LSTM models in multivariate time series prediction tasks.
The researchers proposed a competitive random search method inspired by evolutionary computation to optimize the configuration of parameters within the attention layer.
Experimental results demonstrated that the proposed model achieved competitive prediction performance when compared to other baseline methods.
This highlights the effectiveness of the evolutionary attention learning approach in improving the predictive capabilities of LSTM models for multivariate time series forecasting.

In \cite{Shih2019}, Shih et al. analyzed that the standard attention mechanisms in multivariate time series forecasting reviewed just the information at each previous time step from which to select relevant information to generate the outputs.
For this reason, these mechanisms were unable to effectively capture temporal patterns across multiple time steps.
In the study, they proposed a set of filters designed to extract time-invariant temporal patterns.
Additionally, they developed a novel attention mechanism that not only selects relevant time series but also utilizes frequency domain information for multivariate time series forecasting.
Finally, they applied the proposed approach to many real-world tasks demonstrating that it was able to achieve state-of-the-art performance in most of them.

In \cite{DU2020269}, Du et al. proposed a novel temporal attention encoder-decoder model to successfully deal with multivariate time series forecasting.
They developed an end-to-end DL structure that combined the classical encoder-decoder learning structure with a temporal attention mechanism.
This integrated approach enabled the simultaneous learning of long-term temporal dependency and hidden non-linear correlation features within multivariate temporal data.
Experimental results on five multivariate time series datasets showed that the proposed model had the best forecasting performance compared with baseline models, such as SVR, RNN, CNN, LSTM, and Gated Recurrent Unit (GRU).

\cite{HEIDARI2020626} aimed to predict the energy use of solar-assisted water heating systems using a novel ML approach.
They proposed to use a LSTM network enhanced by an attention mechanism and the decomposition of input data into sub-layers.
They compared the performance of the proposed approach with a feed-forward neural network, a LSTM network, and an Attention-based LSTM neural network.
The experimental results showed that the proposed model outperformed conventional models in this task.

Liu et al. analyzed that the current attention-based recurrent neural networks can effectively represent and learn the dynamic spatiotemporal relationships between exogenous series and target series, but they only perform well in one-step time prediction and short-term time prediction.
In \cite{LIU2020113082}, they proposed dual-stage two-phase-based RNN (DSTP-RNN) for long-term time series prediction.
The DSTP-based structure was designed to enhance spatial correlations between exogenous series.
The first phase of the model generated violent but decentralized response weights, while the second phase led to stationary and concentrated response weights.
Multiple head attention was then employed on the target series to amplify the long-term dependencies.
Experimental results showcased the effectiveness of the proposed model across a range of applications, outperforming nine baseline methods on four datasets from the domains of energy, finance, environment, and medicine.
These findings suggest that the DSTP-RNN model holds promise for developing accurate and robust prediction systems in diverse fields.

Qi et al. analyzed that with the development of attention mechanisms, the frequency and time attention information of audio can be fully exploited, and the amplitude properties of audio can also be better integrated with a good fusion module.
In \cite{10019616}, they improved existing frequency-temporal attention by extracting the attention information with the frequency-temporal attention and performing an additive fusion of features.
Then, they applied attentional feature fusion based on multi-scale channel attention, and finally, temporal dependencies are learned through a self-attention module.
Experimental results on four datasets demonstrated that the proposed model outperformed existing state-of-the-art models.
These findings highlight the effectiveness of the proposed approach in leveraging attention mechanisms and feature fusion for improved audio analysis and prediction tasks.

In \cite{Vaswani2017}, Vaswani et al. proposed the Transformer architecture, based solely on attention mechanisms and fully connected feed-forward networks, with the addition of layer normalization and residual connections.
Experimental results on machine translation tasks showed that this groundbreaking model was superior in quality achieving better results than existing methods being more parallelizable and requiring significantly less time to train.
Moreover, they also showed that the Transformer is able to generalize to other tasks like English constituency parsing.

In \cite{Wu2020}, Wu et al. presented a new approach to time series forecasting by developing a novel method that employs Transformer-based models to forecast time series data.
The approach leveraged self-attention mechanisms to effectively capture and understand complex patterns and dynamics from time series data.
They created a generic framework that can be applied to univariate and multivariate time series data, as well as time series embeddings.
The influenza-like illness (ILI) forecasting was used as a case study.
They showed the effectiveness of the proposed approach and that the produced forecasting results are favorably comparable to the state-of-the-art.

\cite{Zhou2020} discussed the several severe issues of Transformer architecture that prevent it from being directly applicable to long sequence time series forecasting (LSTF), including quadratic time complexity, high memory usage, and the limitation of the encoder-decoder architecture.
To address these issues, Zhou et al. designed an efficient transformer-based model for LSTF, named Informer.
It had some distinctive characteristics, such as a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, the self-attention distilling highlights dominating attention by halving cascading layer input, the generative style decoder predicts the long time series sequences in one forward operation rather than a step-by-step way.
Extensive experiments on four large-scale datasets demonstrated that Informer significantly outperformed existing methods and provided a new solution to the LSTF problem.

\cite{Grigsby2021} analyzed the state-of-the-art models for multivariate time series forecasting.
They discussed that sequence-to-sequence models rely on attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables.
In contrast, graph neural network methods explicitly model variable relationships, however, often rely on predefined graphs and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep.
The study addressed the presented problems by using a Transformer and translating the multivariate forecasting task into a spatiotemporal sequence formulation so that each Transformer input represented the value of a single variable at a given time.
Using this formulation, they were able to let Long-Range Transformers jointly learn interactions between space, time, and value information.
The proposed method, called Spacetimeformer, achieved competitive results on benchmarks of different domains while learning fully-connected spatiotemporal relationships purely from data.

In \cite{LIM20211748}, Lim et al. introduced the Temporal Fusion Transformer (TFT), a novel attention-based architecture able to combine high-performance multi-horizon forecasting with interpretable insights into temporal dynamics.
The presented TFT relied on recurrent layers for local processing and on interpretable self-attention layers for long-term dependencies.
It was able to select relevant features and suppress unnecessary components through a series of gating layers.
Experimental results on a variety of real-world datasets demonstrated significant performance improvements over existing benchmarks and they also highlighted some practical interpretability use cases enabled by the TFT architecture.
TFT architecture was a breakthrough in time series forecasting and achieved state-of-the-art performance in several tasks.
Hereafter, some studies using this architecture are presented.

In \cite{9745215}, the TFT was applied to forecast the trajectory of future vital signs based on time-varying measurements of past vital signs.
This application holds significant importance since the deterioration of a patient’s condition is usually preceded by several hours of abnormal physiology as indicated by the patient’s vital signs.
The model was developed using the Songklanagarind critical care dataset, which includes vital sign measurements from 140 patients.
Experimental results showed that TFT was able to capture the temporal dynamics of vital signs and can potentially be used to detect irregular patterns in vital sign time series.
This suggests that TFT has the potential to assist in the early detection of abnormal physiological patterns and contribute to improved patient care and monitoring in critical care settings.

In \cite{ZHANG2022329}, a TFT was adopted to predict freeway speed with prediction horizons from 5 to 150 minutes.
A traffic speed data set was employed to train and evaluate the TFT prediction model, showcasing the advantages offered by this approach.
The TFT prediction performance was compared with several classic traffic speed prediction methods, and the results revealed that the TFT outperformed the other classic models when the prediction horizon is longer than 30 minutes.
Moreover, the TFT is also more stable when the prediction horizon is 60 minutes or longer.
These findings highlight the effectiveness of TFT in accurately predicting freeway speed and its superiority over traditional methods, especially being stable also for longer prediction horizons.

\cite{WU2022123990} used a novel forecasting approach for interpretable wind speed prediction by incorporating variational mode decomposition (VMD), a TFT model, and an evolutionary algorithm.
In the proposed approach, VMD was employed to break down the raw wind speed sequence into a set of intrinsic mode functions.
Adaptive differential evolution was then used for optimizing several parameters of a TFT allowing it to achieve satisfactory forecasting performance.
Empirical studies using real-world wind speed data sets demonstrated that the proposed model outperformed other comparable models in nearly all performance metrics.
Moreover, TFT allowed gaining information about the importance ranking of the decomposed wind speed sub-sequences, meteorological data, and attention analysis of different step lengths.
These findings underscore the effectiveness and interpretability of the proposed approach in wind speed prediction tasks.


\vspace{0.1 cm}
\subsection{Automated machine learning}
\label{sec:automl}
\vspace{0.1 cm}

In this subsection, an overview of the developed approaches in the AutoML field is presented, with a focus on time series forecasting applications.

\cite{Elshawi2019} emphasized the necessity of automating the process of building good ML models, mainly due to the exponential growth in data volume, which exceeds the capacity of human data scientists.
In this study, Elshawi et al. presented a comprehensive survey that focused on the Combined Algorithm Selection and Hyper-parameter tuning (CASH) problem in the ML domain.
In addition, they highlighted the importance of automating other steps within the ML pipeline, ranging from data understanding to model deployment.
They provided an overview of the state-of-the-art efforts, tools, and frameworks that have been proposed to address these challenges.
Finally, the study discussed various research directions and open challenges that need to be tackled to realize the vision and goals of AutoML, including scalability, optimization techniques, time budget management, and data preparation.

\cite{9579526} observed that data scientists cannot tackle the growing number of challenging tasks due to a lack of expertise and experience across all task domains.
To help address this issue, the study provided a survey of ongoing research in the field of Meta-Learning and AutoML.
It covered AutoML tools such as TransmogrifAI, Auto-Sklearn, AutoGluon, and NNI.
It also summarized possible uses of Meta-Learning for DL, few-shot learning, and automating the ML process.

In \cite{8995391}, Truong et al. conducted a thorough investigation into the current landscape of AutoML tools designed to automate repetitive tasks within ML pipelines.
These tasks include data pre-processing, feature engineering, model selection, hyperparameter optimization, and prediction result analysis.
performed multiple evaluations of these tools using diverse datasets to assess their performance and compare their respective strengths and weaknesses.
The study revealed that while most AutoML tools achieved reasonable results in terms of performance across a wide range of datasets, no single tool managed to outperform all others consistently in all tasks.
Among the tools evaluated, H2O AutoML, Auto-Keras, and Auto-Sklearn demonstrated better performance than Ludwig, Darwin, TPOT, and Auto-ml across various evaluations and benchmarks.

\cite{9033810} reviewed the various AutoML, hyperparameter tuning, and meta-learning approaches available in the literature and pointed out that most of them are neither properly documented nor very clear due to the differences in the approaches.
The strengths and drawbacks of the various approaches and their reviews in terms of algorithms supported, features, and implementations are explored.

\cite{HE2021106622} presented a comprehensive review of the state-of-the-art in AutoML.
He et al. introduced AutoML methods covering data preparation, feature engineering, hyperparameter optimization, and Neural Architecture Search (NAS) with a focus on automating the DL pipeline.
They summarized the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and discussed relevant topics of the NAS methods such as one\slash two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS.
Finally, they discussed some open problems related to the existing AutoML methods for future research such as flexible search space, areas exploration, interpretability, reproducibility, and robustness.

In \cite{Karmaker2021}, Karmaker et al. proposed a classification system for AutoML systems, using a tier schematic to distinguish systems based on their level of autonomy.
They described what an end-to-end ML pipeline actually looks like and analyzed which subtasks have been automated and which are done manually.
In fact, most AutoML systems still require human involvement in some steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising ML technique.
These steps often require a prolonged back-and-forth that makes the process inefficient and keeps AutoML systems from being truly automatic.
They introduced the proposed level-based taxonomy for AutoML systems and defined each level according to the scope of automation support provided.
Finally, they discussed the challenges that stand in the way of automating the whole end-to-end ML pipeline.

In \cite{Chen2021}, Chen et al. described AutoML as a bi-level optimization problem, where one problem is nested within another to search for the optimum in the search space.
They reviewed the current developments of AutoML presenting the state-of-the-art techniques and frameworks in terms of three main categories:
automated feature engineering (AutoFE),
automated model and hyperparameter tuning (AutoMHT),
and Automated Deep Learning (AutoDL).
They concluded by presenting the open challenges of AutoML such as the lack of authoritative benchmarks, efficiency, design of search spaces, and interpretability.

In 2015, Feurer et al. stated that to be effective in practice, ML systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters.
Starting from existing work on efficient Bayesian optimization methods, in \cite{Feurer2015} they presented a robust new AutoML system based on the scikit-learn framework: Auto-Sklearn.
Auto-Sklearn incorporated 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, resulting in a structured hypothesis space with 110 hyperparameters.
One key improvement of Auto-Sklearn was its ability to leverage past performance on similar datasets, taking into account this information during the optimization process.
Furthermore, the system constructed ensembles from the evaluated models.
Experimental results on a wide range of more than 100 diverse datasets demonstrated that Auto-Sklearn substantially outperformed the previous state of the art in AutoML.

In \cite{Jin2019}, Jin et al. proposed a novel framework for efficient NAS enabling Bayesian optimization to guide the network morphism, which essentially keeps the functionality of a neural network while changing its neural architecture.
The framework, called Auto-Keras, employed a neural network kernel and a tree-structured optimization algorithm to facilitate efficient exploration of the search space.
Through extensive experiments conducted on real-world benchmark datasets, they demonstrated the superior performance of the developed framework over the state-of-the-art NAS methods.

In \cite{8955514}, Dyrmishi et al. presented a methodology and a framework for using Meta-Learning techniques to develop new methods that serve as effective decision support for the AutoML process.
Meta-Learning allows learning from previous experience gained during applying various learning algorithms on different types of data and helps reduce the time needed to learn new tasks.
In particular, they used Meta-Learning techniques to answer several crucial questions for the AutoML process such as which classifiers are expected to be the best performing on a given dataset, whether it is possible to predict the training time of a classifier, and which classifiers are worth investing a larger portion of the time budget to improve their performance by tuning them.
In the Meta-Learning process, they used 200 datasets with different characteristics and 30 classifiers from Weka and Scikit-learn libraries.
As a result, this method allowed Meta-Models to be obtained in a fully automated way.

\cite{Gijsbers2019} introduced an open-source benchmark framework for comparing different AutoML systems following best practices and avoiding common mistakes.
The framework is extensible both in terms of AutoML frameworks and tasks.
They used the framework to conduct a thorough comparison of 4 AutoML systems (Auto-WEKA, Auto-Sklearn, TPOT, and H2O AutoML) across 39 datasets.
They analyzed the results and highlighted the need for further AutoML research.
In fact, on some datasets, none of the frameworks outperformed a Random Forest within 4 hours, and high-dimensional or highly multi-class problems were often challenging for AutoML frameworks.

In \cite{computers10010011}, Vaccaro et al. reviewed some ML models and methods proposed in the literature to analyze their strengths and weaknesses.
Then, they proposed their use, alone or in combination with other approaches, to provide possible valid AutoML solutions.
They analyzed these solutions from a theoretical point of view and evaluated them empirically on three Atari games.
The objective of the study was to identify what could be some promising ways to create effective AutoML frameworks able to replace the human expert as much as possible and thereby make the process of applying ML approaches to typical problems of specific domains easier.
The research provided valuable insights into potential directions for future work in the field of AutoML.
The findings contribute to the ongoing efforts to develop robust and efficient AutoML frameworks, paving the way for advancements in automating machine learning processes and reducing the dependence on human expertise.

Nguyen et al. investigated that most AutoML-based Bayesian optimization approaches convert the AutoML optimization problem into a Hyperparameter Optimization (HPO) problem, i.e., by modeling the choice of algorithms as an additional categorical hyperparameter.
They pointed out that using this approach algorithms and their local hyper-parameters are referred to at the same level, and this makes the initial sampling less robust.
In \cite{9660073}, they attempted to formulate the AutoML optimization problem as a Bayesian optimization problem instead of transferring it into a HPO problem.
They proposed a novel initial sampling approach to maximize the coverage of the AutoML search space to help Bayesian optimization construct a robust model.
They tested this approach on 2 independent scenarios of AutoML with 2 operators and 6 operators over 117 benchmark datasets.
Experimental results showed that the performance of Bayesian optimization was significantly improved by using the proposed sampling approach.

In \cite{Feurer2020}, Feurer et al. introduced a new AutoML approach, named PoSH (Portfolio Successive Halving) Auto-Sklearn, which enabled AutoML systems to work well on large datasets under rigid time limits by using a meta-learning technique and a bandit strategy for budget allocation.
They also studied how to let AutoML explore the design space and automatically select the best configuration by itself.
These changes combined give rise to the next generation of their original Auto-Sklearn framework, called Auto-Sklearn 2.0.
They verified the improvements of these additions in an extensive experimental study on 39 AutoML benchmark datasets and compared the results to other popular AutoML frameworks and Auto-Sklearn 1.0, showing that the relative error is reduced by up to a factor of 4.5, and yielding performance in 10 minutes that was better than what Auto-Sklearn 1.0 achieved within an hour.

In \cite{Zimmer2020}, Zimmer et al. presented Auto-PyTorch, a comprehensive framework that enables fully AutoDL by jointly optimizing network architecture, training pipelines, and hyperparameters.
Auto-PyTorch utilizes multi-fidelity optimization techniques along with portfolio construction for warm starting and ensembling of Deep Neural Networks (DNNs), achieving state-of-the-art performance on many tabular benchmarks.
One notable contribution of the study is the introduction of a new benchmark on learning curves for DNNs.
This benchmark provides a valuable evaluation criterion for assessing the effectiveness of AutoDL frameworks.
The authors conducted extensive experiments on typical AutoML benchmarks, demonstrating that Auto-PyTorch outperformed several state-of-the-art competitors in terms of performance.

In their work, the authors of \cite{9534091} conducted a benchmark study to evaluate the performance of eight popular open-source AutoML tools: Auto-Keras, Auto-PyTorch, Auto-Sklearn, AutoGluon, H2O AutoML, rminer, TPOT, and TransmogrifAI.
They also selected twelve widely used OpenML datasets to serve as benchmarks for different machine learning tasks, including regression, binary classification, and multi-class classification.
The study focused on comparing the predictive scores and computational effort of the AutoML tools.
The best predictive results achieved by these tools were compared with the best public results from OpenML.
The comparison revealed that the AutoML tools consistently achieved competitive results, outperforming the best models from OpenML in five of the datasets.
These findings highlight the potential of AutoML tools in fully automating the process of ML algorithm selection and tuning.

In the following, some studies with a focus on the utilization of AutoML for time series forecasting applications are presented.

In \cite{9564380}, a data augmentation method was presented to enhance the performance of neural networks in time series forecasting tasks, especially when limited data is available. 
The proposed method, known as Augmented-Neural-Network, incorporated forecasts from statistical models to achieve competitive results on intermediate-length time series.
The study demonstrated that combining data augmentation with AutoML techniques like NAS can lead to the discovery of suitable neural architectures for specific time series.
By applying this approach, significant improvements in forecasting accuracy were observed on a COVID-19 dataset, with an improvement of approximately 20\% compared to neural networks that did not utilize augmented data.

\cite{su142215292} conducted experiments on time series forecasting using ML, DL, and AutoML techniques.
The datasets used in the experiments were quantitative data of the real prices of the currently most used cryptocurrencies.
The results showed that AutoML for time series is still in the development stage and needs more study to be the main solution to adopt since it was unable to outperform manually designed ML and DL models.

In recent years, there has been significant improvement in the efficiency of AutoDL systems.
However, there has been limited focus on AutoDL frameworks specifically designed for time series forecasting.
In \cite{Deng2022}, Deng et al. proposed an efficient approach to jointly optimize the neural architecture and hyperparameters of the entire data processing pipeline for time series forecasting.
Unlike traditional NAS search spaces, the authors designed a novel NAS space that encompasses various state-of-the-art architectures.
This enables efficient macro-search over different DL approaches specifically tailored for time series forecasting.
To efficiently explore this large configuration space, the researchers employed Bayesian optimization with multi-fidelity optimization techniques.
The study conducted empirical investigations on various forecasting datasets using different budget types, which enabled efficient multi-fidelity optimization.
Moreover, the proposed system, named Auto-PyTorch-TS, was compared against several established baselines.
The results demonstrated that Auto-PyTorch-TS significantly outperformed the baselines across multiple datasets, highlighting its effectiveness in time series forecasting tasks.


\section{Electricity demand forecasting}
\label{sec:demandsoa}
\vspace{0.2 cm}

In this section, the techniques for electricity demand forecasting are presented.
Most of the presented techniques are considered very simple nowadays and they rely on large aggregated data, both on the number of people considered (e.g., the consumption generated by an entire country) or on a very large temporal aggregation (up to 1 year aggregated data).
Our use case is limited to the customers of a small company, from 2 to 4 thousand customers, and it requires forecasts also on an hourly basis for a one-month time horizon.

\cite{singh2013overview} presented a review of electricity demand forecasting techniques.
They classified load forecasting into three categories:
short-term forecasts which are usually from one hour to one week,
medium forecasts which are usually from a week to a year,
and long-term forecasts which are longer than a year.
Based on the various types of studies presented, load forecasting techniques may be presented in three major groups: Traditional Forecasting techniques (regression methods, exponential smoothing, iterative reweighted least-squares), Modified Traditional Techniques (adaptive demand forecasting, Auto-Regressive (AR), Auto-Regressive Moving Average (ARMA), ARIMA, SVM) and Soft Computing Techniques (Genetic Algorithms (GAs), fuzzy logic, neural networks, knowledge-based expert systems).
From the work, it can be inferred that demand forecasting techniques based on soft computing methods are gaining major advantages for their effective use.
There is also a clear move towards hybrid methods, which combine two or more of these techniques.

\cite{TAYLOR200357} investigated the use of weather ensemble predictions in electricity demand forecasting from 1 to 10 days ahead.
They proposed a weather ensemble prediction by considering 51 scenarios for a weather variable.
For each scenario, they produced a scenario for the weather-related component of electricity demand.
The results showed that the average of the demand scenarios is a more accurate demand forecast than that produced using traditional weather forecasts.
The mean of the 51 scenarios is mathematically equivalent to taking the expectation over the weather-related component of the demand probability density function.
They also used the distribution of the demand scenarios to estimate the demand forecast uncertainty.

In \cite{MIRASGEDIS2006208}, Mirasgedis et al. presented how to incorporate weather into models for mid-term electricity demand forecasting.
They studied the daily and monthly electricity demand.
They noticed that the monthly model performs better thanks to the higher level of aggregation but also that the influence of weather on electricity demand is in a more aggregated way and thus may not account well for the influence of unusual or extreme weather on electricity consumption.
The temperature of the day in which electricity demand is projected, the temperature of the two previous days, and the relative humidity have been found to be the most important weather parameters that affect electricity consumption in the Greek interconnected power system.

\cite{5686767} proposed a first use of a neural network with the Backpropagation learning algorithm for Lao state yearly electricity demand forecasting.
They compared it with a regression analysis model showing the higher effectiveness of the neural network.

\cite{5518553} proposed two models for short-term Singapore electricity demand forecasting: the multiplicative decomposition model and the Seasonal ARIMA (SARIMA) model.
Results showed that both models can accurately predict the short-term Singapore demand and that the Multiplicative decomposition model slightly outperforms the SARIMA model.

\cite{8093428} is an empirical study in which some forecasting models are developed for electricity demand using publicly available data and three models based on ML algorithms.
The performance of these models is compared by using different evaluation metrics.
The data consists of several measurements of the electricity market in Turkey from 2011 to 2016 and is available for different time granularities (from hourly to yearly aggregated).
According to the best result of MAPE, electricity demand was predicted with a 1.4 percentage error with the Random Forest model.

In \cite{9046493}, a modeling approach based on association rules was proposed.
Association rules are useful to describe a model in terms of cause and effect.
It did not outperform ARIMA but helped to locate the most frequent patterns of electricity consumption.

In \cite{ALMUSAYLH20181}, Al-Musaylh et al. addressed the short-term electricity demand forecasting with MARS (Multivariate Adaptive Regression Spline), SVR, and ARIMA models using aggregated demand data of Queensland, Australia.
They found out that the MARS and SVR models can be considered more suitable for short-term electricity demand forecasting when compared to the ARIMA model.
As expected, given its linear formulation in the modeling process, the ARIMA model’s performance was lower for all forecasting horizons as it generated very high forecast errors.
The study found that the MARS models were able to provide a powerful, yet simple and fast forecasting framework when compared to the SVR models.

\cite{MA20193433} presented a method based on a SVR to forecast building energy consumption in southern China.
To improve the reliability of SVR in building energy consumption prediction, multiple parameters including weather data (such as yearly mean outdoor dry-bulb temperature, relative humidity, and global solar radiation) and economic factors (such as the ratio of urbanization, gross domestic product, household consumption level and total area of the structure) are taken as inputs.

In \cite{8404313}, a RNN-based time series approach for forecasting Turkish electricity load was proposed.
RNNs, LSTM networks, and GRU networks are used.
The resulting 0.71\% MAPE of their experiments yields better results than existing methods based on ARIMA and ANNs on Turkish electricity load forecasting which have 2.6\% and 1.8\% MAPE respectively.

In \cite{DU2018533} a novel hybrid forecasting system was successfully developed.
It was composed of four modules: data preprocessing module, optimization module, forecasting module, and evaluation module.
In the data preprocessing module, a signal processing approach is employed to decompose, reconstruct, identify, and mine the primary characteristics of the electrical power system time series.
Optimization algorithms are also employed to optimize the parameters of these individual models in the optimization and forecasting modules.
Experimental results showed that the hybrid system can be able to satisfactorily approximate the actual value.
This is an interesting study from which to take inspiration for the system modeling structure, which can be intended for general-purpose and not just for the electrical power use case.

\cite{KIM2019328} proposed a recurrent inception CNN (RICNN) that combines RNN and 1-dimensional CNN (1-D CNN).
They used the 1-D convolution inception module to calibrate the prediction time and the hidden state vector values calculated from nearby time steps.
By doing so, the inception module generates an optimized network via the prediction time generated in the RNN and the nearby hidden state vectors.
The proposed RICNN model has been verified in terms of the power usage data of three large distribution complexes in South Korea.
Experimental results demonstrate that the RICNN model outperforms the benchmarked MLP, RNN, and 1-D CNN in daily electric load forecasting (48-time steps with an interval of 30 minutes).
This is an interesting network structure that can be suited for our specific use case but has to be effective also on a wider time horizon.

In \cite{BEDI20191312}, Bedi and Toshniwal proposed a DL-based framework to forecast electricity demand by taking care of long-term historical dependencies.
In fact, existing methods are only able to handle short-term dependencies.
The proposed approach is called D-FED and is based on a LSTM network and a moving window-based multi-input multi-output mapping approach of active learning.
It is applied to the electricity consumption data of Union Territory Chandigarh, India.
The performance of the proposed approach is evaluated by comparing the prediction results with ANN, RNN, and SVR models.

In \cite{MUZAFFAR20192922}, Muzaffar and Afshari have picked up electrical load data with exogenous variables including temperature, humidity, and wind speed, and used them to train a LSTM network.
It has been shown in this work that LSTM outperforms the other traditional methods such as ARMA, SARIMA, and ARMA with exogenous inputs (ARMAX) reducing the percentage of errors in forecasting the load time series.
From the study, they found out that LSTM can learn the seasonality patterns and the trend as well instead of extracting these features a priori.

In \cite{WEN2020106073}, Wen et al. proposed a DL model to forecast the load demand of aggregated residential buildings with a one-hour resolution, while considering its complexity and variability.
Hourly-measured residential load data in Austin, Texas, USA were used to demonstrate the effectiveness of the proposed model, and the forecasting error was quantitatively evaluated using several metrics.
The used model is a deep RNN model with GRU (DRNN-GRU).
This model assumes knowledge of the future weather data to make a forecast, which would affect the accuracy due to the weather uncertainty over a short to medium period.
The results showed that the proposed model forecasts the aggregated and disaggregated load demand of residential buildings with higher accuracy compared to conventional methods.

\cite{CHITALIA2020115410} presented a robust short-term electrical load forecasting framework that can capture variations in building operation, regardless of building type and location.
Nine different hybrids of recurrent neural networks and clustering are explored.
The test cases involve five commercial buildings of five different building types, i.e., academic, research laboratory, office, school, and grocery store.
Load forecasting results indicate that the DL algorithms implemented in the paper deliver 20-45\% improvement in load forecasting performance as compared to the current state-of-the-art results for both hour-ahead and 24-ahead load forecasting.
It is found that:
(i) the use of hybrid DL algorithms can take as less as one month of data to deliver satisfactory hour-ahead load prediction,
(ii) 15-minute resolution data, if available, delivers a 30\% improvement in hour-ahead load forecasting,
and (iii) the formulated methods are found to be robust against weather forecasting errors.
This study gives useful insights about the quantity and granularity of data needed for a short-term prediction range, these are great findings but do not generalize to longer prediction ranges.

In \cite{WANG2020117197}, Wang et al. proposed a novel approach based on a LSTM network for predicting periodic energy consumption.
They stated that this is novel since general forecasting methods do not concern periodicity.
Hidden features are extracted by the autocorrelation graph among the real industrial data.
Experiments using a cooling system under one-step-ahead forecasting are conducted to verify the performance of LSTM.
It was compared with several traditional forecasting methods, such as the ARMA model, the Auto-Regressive Fractional Integrated Moving Average (ARFIMA) model, and the backpropagation neural network (BPNN).
The Root Mean Square Error (RMSE) of LSTM is 19.7\%, 54.85\%, and 64.59\% lower than BPNN, ARMA, and ARFIMA on the test data.
Furthermore, they demonstrated that the proposed algorithm had the highest generalization capability.

In \cite{WANG2020114561}, Wang et al. proposed a stacking model capable of combining the advantages of various basic prediction algorithms and transforming them into meta-features to ensure that the final model can observe datasets from different spatial and structural angles.
Load data retrieved from two educational buildings in the coastal city of Tianjin, China, is employed for the case study.
The case study buildings are made of classrooms for students and offices for university staff.
Experimental results indicated that the stacking method achieves better performance than other tested ML models (Random Forest, GBDT, Extreme Gradient Boosting (XGBoost), SVM, and KNN) regarding accuracy, generalization, and robustness.

In \cite{SOMU2021110591}, Somu et al. presented kCNN-LSTM, a DL framework that operates on the energy consumption data recorded at predefined intervals to provide accurate building energy consumption forecasts.
kCNN-LSTM employs:
(i) k-means clustering to perform cluster analysis to understand the energy consumption pattern\slash trend;
(ii) CNN to extract complex features with non-linear interactions that affect energy consumption;
and (iii) LSTM network to handle long-term dependencies through modeling temporal information in the time series data.
The performance of kCNN-LSTM was compared with the k-means variant of the state-of-the-art energy demand forecast models in terms of Mean Square Error (MSE), RMSE, Mean Absolute Error (MAE), and MAPE showing the efficiency of kCNN-LSTM model over other models in providing accurate energy consumption demand forecasting.

In \cite{10033079}, an attention-based DL model with interpretable insights into temporal dynamics is presented to forecast short-term loads.
The TFT included a sequence-to-sequence model, which processes the historical and future covariates to enhance the forecasting performance.
A Gated Residual Network (GRN) is applied to drop out unnecessary information and improve efficiency.
The proposed method is tested on anonymized data from a university campus with a time resolution of 30 minutes.
The anomalies and missing data (around 8.85\% of the total data) are imputed with the KNN method.
The testing results demonstrate the effectiveness of the proposed method achieving less than 5\% MAPE.
This is an interesting work on a TFT model, which should be extended to obtain great performance also with a higher prediction range in order to be applicable to the use case treated in this thesis.

\cite{LI2023108743} presented a probabilistic forecasting method for hourly load time series based on an improved TFT (ITFT) model to achieve more accurate and thorough forecasting results.
Hourly load time series was reconstructed into multiple day-to-day load time series at different hour points.
ITFT model replaces the LSTM with a GRU to learn long-term dependence more efficiently.
Quantile constraints and prediction interval (PI) penalty terms were incorporated into the original quantile loss function to prevent quantile crossover and construct more compact PIs.
The results show that the proposed method is explanatory and can significantly improve the reliability and compactness of probabilistic load forecasting results compared with other popular methods, such as Quantile Regression Neural Network (QRNN) and Temporal Convolutional Network (TCN).


\section{Consumption baseline forecasting}
\label{sec:baselinesoa}
\vspace{0.2 cm}

In this section, the techniques for consumption baseline forecasting are presented.
As demonstrated by most of the following papers analyzed, computing forecasts for a single customer is a more complicated use case compared to energy demand forecast over a customer base.
Single customer data is more noisy and very few works consider a single habitation, most of them consider commercial buildings, offices, or schools which reduce the level of noise.

The EU research project S3C developed and tested different guidelines and tools, of particular interest is the guideline on how to create a consumption baseline\footnote{ \url{https://www.smartgrid-engagement-toolkit.eu/fileadmin/s3ctoolkit/user/guidelines/GUIDELINE_HOW_TO_CREATE_A_CONSUMPTION_BASELINE.pdf} }.
The baseline is the reference used to assess the effects of the demand response of a given consumer or set of consumers.
The demand response effect is defined as the difference between the metered consumption and the baseline calculation.
They explained that the baseline calculation method consists of three criteria:
i) data selection method,
ii) estimation method,
and iii) result adjustment.
They pointed out that the combination of these criteria depends on user consumption, weather dependency (including seasonal behavior), and load behavior and should all together fit the user consumption pattern.

In \cite{DEB2017902}, Deb et al. presented a comparison of different time series forecasting techniques for building energy consumption: ANN, ARIMA, SVM, Case-Based Reasoning (CBR), Fuzzy time series, Grey prediction model, Moving average and exponential smoothing (MA \& ES), KNN prediction method and Hybrid models.
Also, hybrid models are reviewed and analyzed, i.e., the combination of two or more forecasting techniques.
The various combinations of the hybrid model are found to be the most effective in time series energy forecasting for single buildings.

\cite{AMBER2018886} aimed to compare the prediction capabilities of five different intelligent system techniques in forecasting the electricity consumption of an administration building.
These five techniques are; Multiple Regression (MR), Genetic Programming (GP), ANN, DNN, and SVM.
The prediction models are developed based on five years of observed data of five different additional parameters such as solar radiation, temperature, wind speed, humidity, and weekday index.
The weekday index is demonstrated as an important parameter that makes it possible to differentiate between working and non-working days.
ANN performs better than all other four techniques with a MAPE of 6\% whereas MR, GP, SVM, and DNN have MAPE of 8.5\%, 8.7\%, 9\%, and 11\%, respectively.

Ahmad et al. in \cite{AHMAD2018301} focused on reviewing data-driven approaches and large-scale building energy predicting-based approaches.
A thorough review of different techniques is presented in the study, including ANN, SVM, clustering-based, statistical, and ML-based approaches.

\cite{FAN2019700} investigated the performance of different strategies for multi-step ahead building energy predictions.
The results of the study demonstrated the potential of recurrent models for short-term building energy predictions.
This study provided references for developing advanced DL models for practical applications.

In \cite{LUSIS2017654}, Lusis et al. studied how calendar effects, forecasting granularity, and the length of the training set affect the accuracy of a day-ahead load forecast for residential customers.
They demonstrated that regression trees, neural networks, and SVR yielded similar average RMSE results, but also that statistical analysis showed that the regression trees technique is significantly better.
The use of historical load profiles with daily and weekly seasonality, combined with weather data, leaves the explicit calendar effects with very low predictive power.
It was also found that one year of historical data is sufficient to develop a load forecast model for residential customers as a further increase in the training dataset has a marginal benefit.
In the setting studied in the paper, it was shown that forecast errors can be reduced by using a coarser forecast granularity.
That is expected since aggregating over time will reduce the variability of the data, as also demonstrated in this thesis, good results on hourly forecasts will be almost impossible to achieve and instead by using a daily forecast results will be more acceptable.

In \cite{7463810}, Kim et al. examined several different data mining techniques and demonstrated GBDT to be an effective method to build baseline electricity usage.
They trained GBDT on data prior to the introduction of new pricing schemes and applied the known temperature following the introduction of new pricing schemes to predict electricity usage with the expected temperature correction.
Their experiments and analyses showed that the baseline models generated by GBDT capture the core characteristics over the two years with the new pricing schemes.
In contrast to the majority of regression-based techniques which fail to capture the lag between the peak of daily temperature and the peak of electricity usage, the GBDT-generated baselines are able to correctly capture the delay between the temperature peak and the electricity peak.
Furthermore, subtracting this temperature-adjusted baseline from the observed electricity usage, they found that the resulting values are more amenable to interpretation, which demonstrates that the temperature-adjusted baseline is indeed effective.
Instead of providing accurate short-term forecasts, their baseline model aims to capture intraday characteristics that persist for years.

In \cite{PLATON201510}, Platon et al. developed predictive models by using ANN and CBR for producing hourly predictions of a building’s electricity consumption.
CRB is based on the concept that the current trend of the building's electrical use can be approximated using past trends occurring at similar conditions.
They showed the supremacy of ANN over CBR in doing the predictions.

In \cite{7576207}, Jie et al. proposed a baseline load forecasting and optimization method based on non-demand-response factors, considering the effects of non-demand-response factors on customer load characteristics and customer baseline load (CBL) forecasting.
The proposed method combines non-demand-response factors mining, similar days selecting, and CBL calculating.
A combined calculation model is adopted to predict the CBL.
The case study reveals the greater accuracy of this method compared to average, linear regression, and neural network methods.

Forecast at the household level is also getting more and more popular in smart building control and demand response programs.
This popularity inspired Dong et al. to develop in \cite{DONG2016341} a hybrid model to address the problem of residential hour and day-ahead load forecasting through the integration of data-driven techniques.
They evaluated five different ML algorithms: ANN, SVR, least-square SVM (LS-SVM), Gaussian process regression (GPR), and Gaussian mixture model (GMM).
They applied these models to four residential data sets obtained from smart meters.
A subdivision of air conditioning (AC) consumption and not-AC was possible and this led to better results with respect to the total consumption.
The final results showed that the hybrid model led to improvements compared to the other ML algorithms for both hour-ahead and 24-h ahead predictions.

In \cite{MOCANU201691}, Mocanu et al. investigated two newly developed stochastic models for time series prediction of energy consumption, namely Conditional Restricted Boltzmann Machine (CRBM) and Factored Conditional Restricted Boltzmann Machine (FCRBM).
The assessment of the two models is made on a benchmark dataset consisting of almost four years of one-minute resolution electric power consumption data collected from an individual residential customer.
As the prediction horizon is increasing, FCRBMs and CRBMs seem to be more robust and their prediction error is typically half that of the ANN.
In addition from other experiments, it can be observed that all methods perform better when predicting the aggregated active power consumption, than predicting the demand of intermittent appliances (e.g., electric water heater) recorded from sub-meterings.

In \cite{ALOBAIDI2018997}, a robust ensemble model was proposed to predict day-ahead mean daily electricity consumption on the household level.
The proposed ensemble learning strategy utilized a two-stage resampling plan, which generated diversity-controlled but random resamples that were used to train individual ANN members.
Experimental results on a case study showed that the proposed ensemble is able to generate better estimates compared to ANN models and the Bagging ensemble.

To counter the high nonlinearity between inputs and outputs of building energy consumption prediction models, in \cite{ZHONG2019403} a novel vector field-based SVR method is proposed.
Through multi-distortions in the sample data space or high-dimensional feature space mapped by a vector field, the optimal feature space is found, in which the high nonlinearity between inputs and outputs is approximated by linearity.
A large office building in a coastal town in China is used for a case study, and its summer hourly cooling load data are used as energy consumption data.
The proposed method ensures high accuracy, generalization ability, and robustness for building energy consumption prediction.

In \cite{SHAO2020102128}, Shao et al. studied and analyzed the energy consumption of hotel buildings by developing a SVM energy consumption prediction model.
The SVM model took as input variables the weather parameters and operating parameters of the hotel air-conditioning system.
They selected as the kernel function the RBF (Radial Basis Function) kernel function and optimized the parameters of the kernel for finding the best accuracy for the model predictions.
The MSE value of the final model prediction in the case study was 2.22\% and R\textsuperscript{2} (coefficient of determination) was 0.94.
This use case is different from the standard single-building forecasts since a hotel includes different rooms and aggregates the consumption over them, then it is more influenced by periods of the year where there could be more or fewer customers that require different levels of demand.
Moreover, in this case, hotel parameters like the air conditioning system are available and help in reducing the overall forecast error.

In \cite{WANG201910}, a probabilistic load forecasting method for individual consumers is proposed to handle the variability and uncertainty of future load profiles.
Pinball loss-guided LSTM network is used to model both the long-term and short-term dependencies within the load profiles.
Forecasting for both residential and commercial consumers is tested.
Experimental results over different customers showed that the proposed method had superior performance over traditional methods such as QRNN, GBDT, and traditional LSTM.

\cite{CAI20191078} aimed to use DL-based techniques for day-ahead multi-step load forecasting in commercial buildings.
The RNN and CNN models have been proposed and formulated under both recursive and direct multi-step manners.
The performance is compared with the SARIMA with exogenous inputs (SARIMAX) model.
The gated 24-h CNN model, performed in a direct multi-step manner, proves itself to have the best performance, improving the forecasting accuracy by 22.6\% compared to that of the SARIMAX, demonstrating the supremacy of DL models in this kind of use cases.

In \cite{KIM201972}, Kim and Cho proposed a CNN-LSTM neural network for extracting spatial and temporal features to effectively predict housing energy consumption.
This is demonstrated to be an effective architecture since the CNN layer can extract the features between several variables affecting energy consumption, and the LSTM layer is able to model temporal information of irregular trends in time series components.
The CNN-LSTM method achieved very good prediction performance for housing energy consumption that previously was even difficult to predict.
It recorded the smallest value of root mean square error compared to the conventional forecasting methods for the dataset on individual household power consumption.
It managed to predict complex electric energy consumption and obtained the highest performance in all cases of minutely, hourly, daily, and weekly unit resolutions compared to other methods.
They stated also that having household characteristics such as occupancy and behavior of the residents might have a large influence on predicting electric energy consumption and could improve the performance of the model.

In \cite{SOMU2020114131}, Somu et al. proposed a hybrid model for building energy consumption forecasting using LSTM networks.
In particular, they presented eDemand, which is an energy consumption forecasting model which employs LSTM networks and an improved sine cosine optimization algorithm for accurate and robust building energy consumption forecasting.
Live energy consumption data was obtained from an academic building, the Indian Institute of Technology in Bombay, and it is used to forecast short-term, mid-term, and long-term energy consumption.
The conducted experiments revealed that the proposed model outperforms the state-of-the-art energy consumption forecast models according to different evaluation metrics.

Also, a study on Deep Reinforcement Learning (DRL) techniques for building energy consumption forecasts was proposed in \cite{LIU2020109675}.
Very little is known about DRL techniques in forecasting building energy consumption.
A case study of an office building was presented and three commonly-used DRL techniques to forecast building energy consumption are used: Asynchronous Advantage Actor-Critic (A3C), Deep Deterministic Policy Gradient (DDPG), and Recurrent Deterministic Policy Gradient (RDPG).
The objective of the paper was to investigate the potential of DRL techniques in building energy consumption predictions.
A comprehensive comparison between the proposed DRL models and common supervised models was provided.
Experimental results showed that DDPG outperformed supervised models both in single-step ahead prediction and multi-step ahead prediction.
RDPG model did not have advantages over DDPG in single-step ahead prediction, yet led to evident accuracy improvement in multi-step ahead prediction.
A3C led to poor performance both in single-step ahead prediction and multi-step ahead prediction, indicating that the technique is not adequate for forecasting building energy consumption.

Producing a model for every single customer is not very scalable (high computational complexity) and a model that is able to use the information about a customer as features and can generalize over a customer base would be the best solution for an energy company.
The next few recent works try to use novel and advanced techniques to try out this approach.

A novel deep ensemble learning-based probabilistic load forecasting framework is proposed in \cite{YANG2019116324} to quantify the load uncertainties of individual customers.
The presented framework employed the profiles of different customer groups and integrated them into the understanding of the task.
Specifically, customers are clustered into separate groups based on their profiles and a multitask representation learning approach is employed on these groups simultaneously.
This technique led to better feature learning across groups and it was particularly useful to improve the performance of predicting residential demand response and managing home energy in smart grids.
However, this technique in order to be applicable needs customers’ personal information and a large customer base to be effective.

\cite{ZANG2021120682} proposed a novel day-ahead residential load forecasting method based on feature engineering, pooling, and a hybrid DL model.
Feature engineering was performed using two-stage preprocessing on data from each user, i.e., first decomposition and then multi-source input dimension reconstruction.
Subsequently, the pooling operation was adopted to merge data from both the target user and its interconnected users, in a descending order based on mutual information.
Finally, a hybrid model with two input channels was developed by combining LSTM with a self-attention mechanism.
The case studies were conducted on a practical dataset containing multiple residential users.
The proposed load forecasting method achieved the best performance with a four-user data pool, 49 time steps, and 24 feature dimensions.
The optimal performance corresponded to 15.33\%, 56.86 kW, and 82.50 kW in terms of MAPE, MAE, and RMSE, respectively.
The proposed method was demonstrated to be an effective choice for day-ahead residential load forecasting.

\cite{NAZIR2023100888} proposed a daily, weekly, and monthly energy consumption prediction model for single customers by using a TFT.
The study used a TFT which considered both primary and valuable data sources, and batch training techniques.
A data set of 169 customers were considered in the study and the TFT was tested on data from only one customer for demonstrating the effectiveness of the customer-based predictive model, which has the advantage that it does not depend on overall energy consumption.
The model’s performance has been related to the LSTM, interpretable LSTM, and TCN models.
The overall symmetric MAPE (sMAPE) of LSTM, interpretable LSTM, TCN, and proposed TFT remained at 29.78\%, 31.10\%, 36.42\%, and 26.46\%, respectively.
The sMAPE of the TFT has proved that the model has performed better than the other DL models.
This is a very smart recent work in which by using time series of also different customers can improve the quality of forecasts with respect to other models for a single customer.


\section{Electricity production forecasting}
\label{sec:productionsoa}
\vspace{0.2 cm}

Electricity production forecasting is a classic problem in the time series field.
In particular, recently the importance of renewable energy sources has been raised due to the clear effects of climate change.
Understanding how much energy the systems based on renewable energy sources can produce with respect to their maximum production capacity is crucial.
However, this is quite difficult to forecast since it depends on future natural phenomena.
Different studies in the literature distinguish between different renewable energy sources, such as photovoltaic (PV), wind, geothermal, biomass, and hydropower.
In this section, the techniques for PV electricity production forecasting are presented.

PVGIS\footnote{ \url{https://joint-research-centre.ec.europa.eu/pvgis-online-tool_en} } is a tool of the EU-Joint Research Center that provides information about solar radiation and PV system performance for any location in Europe and Africa, as well as a large part of Asia and America.
PVGIS uses high-quality solar radiation data obtained from satellite images, as well as ambient temperature and wind speed from climate reanalysis models.
It is a free tool that allows specifying the details of a PV plant to obtain its potential generation.
It is useful to get an indication of the potential generation but it is not accurate as the methods presented in the literature.
In fact, specific methods can focus on specific plants taking into account historical data from which to extract the specific production performance and consider the most recent weather forecasts available.

\cite{INMAN2013535} reviewed the theory behind the forecasting methodologies, and presented several successful applications of solar forecasting methods for both the solar resource and the power output of solar plants at the utility-scale level.
Some examples of the presented approaches are Regressive methods, ANNs, Numerical Weather Prediction (NWP), and hybrid methods incorporating two or more techniques.

Zamo et al. in 2014 presented a pair of articles proposing a benchmark of statistical regression methods for short-term forecasting of PV electricity production.
The first one treated deterministic forecasts of hourly production \cite{ZAMO2014792}, and the second one probabilistic forecasts of daily production \cite{ZAMO2014804}.
The proposed benchmark designated Random Forest as the best forecast model for hourly PV production with a short lead time (from 28 to 45 h).
Their results also suggested that the RMSE can be reduced to about 5.8\% by first forecasting the production for each individual power plant and then summing these forecasts up.
For probabilistic forecasts of 2 days ahead daily production, quantile regression (QR) based forecasts performed significantly better than the climatology, with a CRPS (continuous ranked probability score) lowered by up to 50\%.
For most power plants, a QR-based forecast performs better than the others.
But the most accurate forecast may vary from one power plant to another and with the number of forecast quantiles.

\cite{ANTONANZAS201678} presented a review of PV power forecasting up to 2016.
Forecasting techniques such as regressive methods, ANN, KNN, SVM, Random Forest, and hybrid methods are presented.
Most recent papers used ML techniques, due to the ease of modeling without the need of knowing PV plant characteristics, since these approaches are able to learn them from data.
Also, spatial and temporal horizons and performance metrics are discussed.
The forecast horizon where most research has been done is the day ahead.
The reason is that most of the energy is traded in day-ahead markets when planning and unit commitment takes place.
They discussed that spatial averaging is very useful since it reduces the variability of the solar resource and generates regional forecasts that are more reliable than single-site ones.
This is caused by the smoothing effect, which cancels errors with opposite signs in different PV plants.

Barbieri et al. in \cite{BARBIERI2017242} found out that ANNs and SVM are appropriate approaches for short-term horizons and NWP are better suited for longer horizons.
In fact, while a probabilistic method based on historical data may be valuable for very long-term forecasts, such an approach cannot take into consideration the complex variations of the cloud cover causing short-term sunlight disruptions.
Only a deterministic atmospheric modeling approach can deal with the stochastic changes of solar radiance during the day.
Within this type of model, NWP data-based models are well adapted for day-ahead forecasts but suffer from a too coarse temporal resolution.
Sky imagers are a precious tool to identify cloud types and anticipate the impact of shading on PV power generation.
They concluded by introducing some future works such as the elaboration of algorithms that can calculate cloud cover and classify clouds using online data and a fine sampling period.
In addition, measuring precisely the effects of each type of cloud on solar irradiance could greatly help in improving the results.

In \cite{DAS2018912}, Das et al. based on the studies dated up to 2018 found out that ANN and SVM-based forecasting models performed well under rapid and varying environmental conditions.
In addition, most of the studies adopted numerous techniques to develop their forecasting model to obtain better accuracy.
Moreover, a considerable number of studies classified the forecasted day into different categories based on the weather conditions using several techniques and then developed the forecasting model.
However, the range of the observed error was remarkably high due to different weather conditions.
For performing good predictions and minimizing errors, the separate sub-model for each weather condition has to perform well.

\cite{SOBRI2018459} classified solar PV forecasting methods into three major categories, that are time series statistical, physical, and ensemble methods.
Among the most used methods, there are ANN and SVM thanks to their ability in solving complex and non-linear forecasting models.
The metrics assessment shows that Artificial Intelligence (AI) models could decrease the error compared to other statistical approaches.
The ensemble methods have been introduced recently and thanks to their ability to merge linear and non-linear techniques enhanced the accuracy and performance in comparison with individual models.
The standard metrics that are used for evaluating solar prediction accuracy were presented for specific applications to allow the selection of the appropriate solar forecasting approaches to ensure better performance.

\cite{DEFREITASVISCONDI201954} presented a literature review on big data models for solar PV electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-the-art techniques to the problem.
They also included the motivation behind each project proposal, and the characteristics and quality of data used to address the problem, among other issues.
They affirmed that the prediction of solar electricity generation is currently an ongoing academic research question.
ML is widely used, and approaches based on neural networks are considered the most accurate.
Extreme learning machine (ELM) was also a great addition and it has reduced training time and raised precision.

\cite{AHMAD2018465} investigated the accuracy, stability, and computational cost of Random Forest and Extra Trees techniques for predicting the hourly PV generation output.
They compared the performance of the proposed methods with SVR.
They proved that all developed models have comparable predictive power and are equally applicable for predicting hourly PV output.
Despite their comparable predictive power, Extra Trees outperformed Random Forest and SVR in terms of computational cost.
They concluded by affirming that the stability and algorithmic efficiency of Extra Trees makes this technique an ideal candidate for wider adoption in PV output forecasting.

In \cite{AHMED2020109792}, Ahmed et al. reviewed and evaluated contemporary PV solar power forecasting techniques.
They noticed through correlation analysis that solar irradiance is the most correlated feature with PV output and consequently weather classification and cloud motion study are crucial operations for optimal results.
In addition, they stated that the best data-cleaning processes are normalization and wavelet transforms, and that augmentation using generative adversarial networks is recommended for network training and forecasting.
Furthermore, they also analyzed that the optimization of inputs and network parameters can be done by using GAs and particle swarm optimization.
They determined that ensembles of ANNs are the best approach for forecasting short-term PV power.

\cite{9848724} examined the performance of the LSTM method in Turkey's electricity production estimation and determined the optimization technique that provides the best performance in the LSTM estimation method.
It was observed that the energy production estimation of LSTM and Adam optimization technique achieved successful results.

In \cite{GELLERT2019546}, Gellert et al. proposed and evaluated a context-based technique to anticipate electricity production and consumption in buildings.
They focused on a household with PVs and an energy storage system.
They analyze the efficiency of Markov chains, stride predictors, and also their combination into a hybrid predictor in modeling the evolution of electricity production and consumption.
Experimental results showed that the best predictor is the Markov chain configured with an electric power history of 100 values, a context of one electric power value, and an interval size of 1.

A GA-based SVM (GASVM) model for short-term power forecasting of residential-scale PV systems is proposed in \cite{VANDEVENTER2019367}.
The GASVM model classified the historical weather data using an SVM classifier initially and later it is optimized by the GA using an ensemble technique.
Experimental results demonstrated that the proposed GASVM model outperformed the conventional SVM model by a difference of about 669.624W in the RMSE value and 98.7648\% of the MAPE error.

In \cite{ZHOU2020117894}, a hybrid model (SDA-GA-ELM) based on ELM, GA, and customized similar day analysis (SDA) has been developed to predict hourly PV power output.
In the SDA, the Pearson correlation coefficient is employed to measure the similarity between different days based on five meteorological factors, and the data samples similar to those from the target forecast day are selected as the training set of ELM.
In the ELM, the optimal values of the parameters are searched by the GA to improve the prediction accuracy.
The results show that the SDA-GA-ELM model has higher accuracy and stability than other tested approaches in day-ahead PV power prediction, such as ELM, SVM, SDA-ELM, and SVM-ELM.

\cite{9248865} presented case studies on forecasting PV power production and electricity demand in Portugal.
They studied an ensemble of different ML methods (SVM, Random Forest, LSTM, and ARIMA) to exploit the growing collection of energy supply and demand records.
The ensemble used only electricity data to forecast since only this data is available online for any forecasting horizon.
The ensemble method was based on offline training and online forecasting, by applying the most recent power measurements to trained models.
The different ML methods performed different non-linear transformations to the same electricity data, thus introducing diversity in the ensemble.
To assess the forecasting performance of the system, they considered two forecasting horizons relevant to the Internal Electricity Market, namely 36 hours ahead, relevant to the single day-ahead coupling, and 2 hours ahead, relevant to the single intraday coupling.
The forecasting performance using only electricity data is compared with state-of-the-art models and improves the reference accuracy in their case studies.
Since the ensemble relies only on energy data, the results showed that ML methods are useful to exploit energy big data for efficient energy forecasting systems.
As demonstrated by other papers, weather information such as solar irradiance has a high correlation with energy production, and when studying a few plants, instead of all the ones in Portugal, this information has a relevant impact on performance.

A novel hybrid method for deterministic PV power forecasting based on wavelet transform (WT) and CNN is proposed in \cite{WANG2017409}.
WT is used to decompose the original signal into different frequency series.
CNN is employed to extract the nonlinear features and invariant structures exhibited in each frequency.
A probabilistic PV power forecasting model that combines the proposed deterministic method and spine quantile regression is developed to statistically evaluate the probabilistic information in PV power data.
Statistical results showed that the average MAPE, RMSE, and MAE of the proposed deterministic model outperformed the compared benchmarks in terms of seasons, forecasting horizons, and PV power locations.

Day-ahead power output time series forecasting methods are proposed in \cite{GAO2019115838}, in which the ideal weather type (sunny day) and non-ideal weather types (rainy days, windy days, and foggy days) have been separately discussed.
For ideal weather conditions, a forecasting method is proposed based on meteorology data of the next day using LSTM networks.
For non-ideal weather conditions, time series relevance and specific non-ideal weather type characteristics are considered in the LSTM model by introducing adjacent day time series and typical weather type information.
Specifically, daily total power, which is obtained by the discrete grey model (DGM), is regarded as input variables and applied to correct power output time series prediction.
Prediction performance comparison between proposed methods with traditional algorithms revealed that the RMSE accuracy of forecasting methods based on LSTM networks can reach 4.62\% for ideal weather conditions.
For non-ideal weather conditions, the dynamic characteristic is effectively described by proposed methods and the proposed methods obtained superior prediction accuracy.
This is an interesting study on how to treat the problem in two subcases, however,  for real applications, a single network able to adapt to weather conditions by itself is needed.

In \cite{WANG2019113315}, a CNN, a LSTM network, and a hybrid model based on CNN and LSTM models were proposed by Wang et al and they are applied to the data of the DKASC (Dattajirao Kadam Arts, Science and Commerce) college PV system.
The results showed that when the input sequence is increased, the accuracy of the model is also improved, and the prediction effect of the hybrid model is the best, followed by that of the CNN.
While the LSTM network had the worst prediction effect, the training time was the shortest.

In \cite{WANG2019116225}, a hybrid DL model (LSTM-CNN) is proposed and applied to PV power prediction.
In the proposed hybrid prediction model, the temporal features of the data are extracted first by the LSTM network, and then the spatial features of the data are extracted by the CNN model.
The results showed that the hybrid prediction model had a better prediction effect than the single prediction models in isolation, and the proposed hybrid model is also better than the CNN-LSTM (extract the spatial characteristics of data first, and then extract the temporal characteristics of data).

A hybrid DL model combining wavelet packet decomposition (WPD) and LSTM networks is proposed in \cite{LI2020114216}.
The hybrid DL model is utilized for one-hour-ahead PV power forecasting with five-minute intervals.
WPD is first used to decompose the original PV power series into sub-series.
After the decomposition, four independent LSTM networks are developed for the sub-series.
Finally, the results predicted by each LSTM network are reconstructed and a linear weighting method is employed to obtain the final forecasting results.
Experimental results show that the proposed hybrid DL model exhibits superior performance in both forecasting accuracy and stability with respect to LSTM, RNN, GRU, and MLP.

In \cite{MELLIT2021276}, different kinds of neural networks for short-term output PV power forecasting have been developed and compared: LSTM, Bidirectional-LSTM (BiLSTM), GRU, Bidirectional-GRU (BiGRU), One-Dimension CNN (CNN1D), as well as other hybrid configurations such as CNN1D-LSTM and CNN1D-GRU.
A database of the PV power produced by the microgrid installed at the University of Trieste (Italy) is used to train and comparatively test the neural networks.
The performance has been evaluated over four different time horizons, for one-step and multi-step ahead.
The results show that the investigated neural networks provide very good accuracy, particularly in the case of a 1-minute time horizon with one-step ahead (correlation coefficient is close to 1), while for the case of multi-step ahead (up to 8 steps ahead) the results are found to be acceptable (correlation coefficient ranges between 96.9\% and 98\%).
The new advanced neural network algorithms are able to lead to acceptable accuracy in the case of cloudy days.
However, even though the models have great potential for fine time granularity, these time ranges are quite limited and they should be proved to be effective also on longer ranges to be used by PV plant owners.

\cite{en15145232} aimed to predict hourly day-ahead PV power generation by applying the TFT model.
TFT incorporates an interpretable explanation of temporal dynamics and high-performance forecasting over multiple horizons.
The proposed forecasting model has been trained and tested using data from six different facilities located in Germany and Australia.
The results have been compared with other algorithms like ARIMA, LSTM, MLP, and XGBoost.
The use of TFT has been shown to be more accurate than the rest of the algorithms in forecasting PV generation in the tested different facilities.
The importance of the decoder and encoder variables has been also calculated, revealing that solar horizontal irradiation and the zenith angle are the key variables for the model.
This model was proved to be effective also in this task, showing the superiority of the TFTs over standard DL models, even though it is quite more complex and requires a larger amount of data to achieve good prediction results.
