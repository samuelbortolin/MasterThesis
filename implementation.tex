\chapter{Prototype Implementation}
\label{cha:implementation}
\vspace{0.4 cm}

This chapter presents which components of the system have been implemented and how.
Only a subset of the components of the designed architecture in chapter~\ref{cha:system} have been implemented.
This is because a prototype was developed as a Proof of Concept (PoC) with a focus on key components for validating the core system functionalities for each specific use case.
These are the training of models, the forecast of new data, and the evaluation of the performance of the developed models.
The remaining components were not implemented since they were not crucial for having a working system, in fact, the overall architecture was designed in order to build a Software as a Service (SaaS) on the implemented core system functionalities.

The first section describes the implementation of the system's common components across the various specific use cases and the others explain the implementation details of the use case-specific modules.
After this chapter, it will be clear how the system prototype was implemented and ready for validation and testing phases, which are discussed in chapter~\ref{cha:evaluation}.


\section{System's common components}
\label{sec:componentsimpl}
\vspace{0.2 cm}

The effectively implemented components of the architecture designed for the proposed system are reported in figure~\ref{fig:implementationcomponents}.
The interactions among them are reported in figure~\ref{fig:implementationinteractions}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/implementation_components}
\caption{The effectively implemented components of the architecture designed for the proposed system.}
\label{fig:implementationcomponents}
\end{figure}

The ML model interface was implemented using the pickle library\footnote{ \url{https://docs.python.org/3/library/pickle.html} } for interfacing with the ML models storage for storing the ML models and retrieving the stored ones making them available for prediction and evaluation.
The use of the MLflow client\footnote{ \url{https://mlflow.org/} } was thought of as a possible interface in the complete system integration for interfacing with the ML models storage.

The model training task trains new models based on available data for the specific use case.
The details about the models and their training process for each specific use case are reported in the dedicated sections.

The forecasting task forecasts future data using the available models for the specific use case.
The details about the models and their forecasting process for each specific use case are reported in the dedicated sections.

The performance evaluation task evaluates the performance of the available models for the specific use case.
The details about the performance evaluation process for each specific use case are reported in chapter~\ref{cha:evaluation}.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/implementation_interactions}
\caption{The interactions among the effectively implemented components of the architecture designed for the proposed system.}
\label{fig:implementationinteractions}
\end{figure}

The system interacts with the following external components: data storage, weather data storage, ML models storage, and forecasts storage.

The data storage consists of CSV files containing the data for the different use cases, which are loaded using the pandas library\footnote{ \url{https://pandas.pydata.org/} }.
InfluxDB\footnote{ \url{https://www.influxdata.com/} } was thought of as a possible database in the complete system integration for managing the data.

The weather data storage consists of a JSON file containing the weather data, which are loaded using the pandas library.
Weather data is manually obtained from the Weatherbit\footnote{ \url{https://www.weatherbit.io/} } APIs.
InfluxDB was thought of as a possible database in the complete system integration for managing the weather data with an automatic download of weather data using Weatherbit APIs.

The ML models storage consists of pickle files containing the ML models.
MLflow was thought of as a possible ML model storage in the complete system integration for managing the ML models.

The forecasts storage consists of CSV files containing the forecasts, which are stored using the pandas library.
InfluxDB was thought of as a possible database in the complete system integration for managing the forecasts.


\section{Electricity demand forecasting models}
\label{sec:demandimpl}
\vspace{0.2 cm}

The electricity demand forecasting models rely on dedicated data preprocessing which consists of parsing the aggregated consumption data over the customers from a CSV file using the pandas library.
Then, this basic data is enhanced with the air temperature, apparent temperature, and relative humidity parsed from the weather JSON file, in which the reported value is the average of the value in the last hour.
Finally, data is cleaned up by filling in the gaps using the linear interpolation provided by the pandas library, which proved to be effective since there was just some missing data in the weather information.
Two possible granularities are considered: hourly and daily.
The data is with an hourly granularity, so for the daily granularity, data is also aggregated over the day summing the active incomings of the single hours, and averaging the weather data over the day.

The developed models are some baseline approaches that consider repeating past days and weeks, an ARIMA model, a support vector regressor model, a hist gradient boosting regressor model, an extreme gradient boosting regressor model, a prophet model, a LSTM model, a GRU model, a CNN model, some models derived by the combination of the previous techniques, a TFT model, and an AutoML approach.

The baseline approaches are developed using the NumPy library\footnote{ \url{https://numpy.org/} }.
In particular, the tile method is used in different ways depending on the considered baseline.
For the one-day baseline, the last day of the training active incomings is replicated for the days of the test set.
For the one-week baseline, the last week of the training active incomings is replicated for the weeks of the test set.

The ARIMA model is developed using the pmdarima library\footnote{ \url{http://alkaline-ml.com/pmdarima/} }.
The auto\_arima method is used to automatically discover the optimal order for an ARIMA model to better fit the training active incomings.
For the prediction, it simply takes in input the length of the test set and computes the predictions.

Support vector regressor and hist gradient boosting regressor models are developed using the scikit-learn library\footnote{ \url{https://scikit-learn.org/} }.
The extreme gradient boosting regressor model is developed using the XGBoost library\footnote{ \url{https://xgboost.readthedocs.io/} }.
Support vector regressor, hist gradient boosting regressor, and extreme gradient boosting regressor models take advantage of the following features for the hourly granularity: the hour, the day, the weekday, the month, and the year information of the time instant for which to compute the prediction, the air temperature, the apparent temperature, and the relative humidity in the considered hour.
In addition, the active incomings of the past 24 hours are also used.
For the daily granularity instead, the hour information of the date for which to compute the prediction is not considered as a feature, the weather information is obtained as the average over the day, and in place of the active incomings of the past 24 hours are used the daily active incomings of the past 14 days.
For the prediction, one time instant at a time is predicted using weather forecasts, and the predicted demand data is used as previous information for the next time instants.

The prophet model is developed using the prophet library\footnote{ \url{https://facebook.github.io/prophet/} }.
The fit method tasks as input a pandas data frame with just the information related to the time instants and the training active incomings and fits the optimal prophet model.
For the prediction, it takes as input a pandas data frame with the time instants for which to compute the predictions and computes them, providing also uncertainty intervals.

LSTM, GRU, and CNN models are developed using the Keras library\footnote{ \url{https://keras.io/} }.
These models take advantage of the following features for the hourly granularity: the hour, the day, the weekday, the month, and the year information of the time instant for which to compute the prediction, the air temperature, the apparent temperature, and the relative humidity in the considered hour.
In addition, the active incoming of the previous hour is also used.
Since these models are able to deal with sequences, a look back at the past 3 hours and at the same hour over the past 14 days is considered and passed as input to the models with the same features as the time instant for which to compute the prediction but with the actual consumption data of the past time instants.
For the daily granularity instead, the hour of the date for which to compute the prediction is not considered as a feature, the weather information is obtained as the average over the day, and the considered look back is at the past 14 days.
For the prediction, one time instant at a time is predicted using weather forecasts, and the predicted demand data is used as previous information for the next time instants.

The TFT model is developed using the PyTorch Forecasting library\footnote{ \url{https://pytorch-forecasting.readthedocs.io/} }.
A TimeSeriesDataSet is constructed using the following features for the hourly granularity: the time index, the hour, the day, the weekday, the month, and the year information of the time instant for which to compute the prediction, the air temperature, the apparent temperature, and the relative humidity in the considered hour.
For the daily granularity instead, the hour of the date for which to compute the prediction is not considered as a feature, and the weather information is obtained as the average over the day.

The AutoML approach is based on the TimeSeriesForecastingTask of the Auto-PyTorch library\footnote{ \url{https://automl.github.io/Auto-PyTorch/master/} }.
The AutoML approach takes advantage of the following features for the hourly granularity: the hour, the day, the weekday, the month, and the year information of the time instant for which to compute the prediction, the air temperature, the apparent temperature, and the relative humidity in the considered hour.
For the daily granularity instead, the hour of the date for which to compute the prediction is not considered as a feature, and the weather information is obtained as the average over the day.


\section{Consumption baseline forecasting models}
\label{sec:baselineimpl}
\vspace{0.2 cm}

Describe the implementation of the consumption baseline forecasting models ...
\begin{itemize}
  \item data preprocessing: consists of parsing single customer consumption data from a CSV file, then basic data is enhanced with the following weather information parsed from JSON file, the reported value is the average of the value in the last hour: air temperature, apparent temperature, and relative humidity. Finally, data is cleaned by filling the gaps by using a linear interpolation;
  \item models training: describe the training for all the specific models;
  \item prediction: describe the prediction for all the specific models.
\end{itemize}


 - TODO describe what the models take in input for training/forecasting -


The developed models are the following: some baseline approaches which consider repeating past days and weeks, an ARIMA model, a support vector regressor model, a hist gradient boosting regressor model, an extreme gradient boosting regressor model, a prophet model, a LSTM model, a GRU model, a CNN model, a combination of the previous techniques, a TFT model, and an AutoML approach.

The baseline approaches are developed using the NumPy library.
The ARIMA model was developed using the pmdarima library.
Support vector regressor and hist gradient boosting regressor models were developed using the scikit-learn library.
The extreme gradient boosting regressor model was developed using the XGBoost library.
The prophet model was developed using the prophet library.
LSTM, GRU, and CNN models were developed using the Keras library.
The TFT model was developed using the PyTorch Forecasting library.
The AutoML approach was based on Auto-PyTorch.


\section{Electricity production forecasting models}
\label{sec:productionimpl}
\vspace{0.2 cm}

Describe the implementation of the electricity production forecasting models ...
\begin{itemize}
  \item data preprocessing: consists of parsing single PV plant production data from a CSV file, aggregating the single PV plant data to obtain the aggregated production data over the PV plants, then basic data is enhanced with the following weather information parsed from JSON file, the reported value is the average of the value in the last hour: air temperature, apparent temperature, relative humidity, wind speed, wind direction, pressure altimeter, visibility, sky coverage, diffuse horizontal irradiance, direct normal irradiance, global horizontal irradiance, solar radiation, UV index, solar elevation angle, and solar azimuth angle. Finally, data is cleaned by filling the gaps by using a linear interpolation;
  \item models training: describe the training for all the specific models;
  \item prediction: describe the prediction for all the specific models.
\end{itemize}


 - TODO describe what the models take in input for training/forecasting -


The developed models are the following: some baseline approaches which consider repeating past days and weeks, an ARIMA model, a support vector regressor model, a hist gradient boosting regressor model, an extreme gradient boosting regressor model, a prophet model, a LSTM model, a GRU model, a CNN model, a combination of the previous techniques, a TFT model, and an AutoML approach.

The baseline approaches are developed using the NumPy library.
The ARIMA model was developed using the pmdarima library.
Support vector regressor and hist gradient boosting regressor models were developed using the scikit-learn library.
The extreme gradient boosting regressor model was developed using the XGBoost library.
The prophet model was developed using the prophet library.
LSTM, GRU, and CNN models were developed using the Keras library.
The TFT model was developed using the PyTorch Forecasting library.
The AutoML approach was based on Auto-PyTorch.
