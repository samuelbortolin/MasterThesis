\chapter{Performance Evaluation}
\label{cha:evaluation}
\vspace{0.4 cm}

In this chapter, the proposed system is validated and the performance of the models for the different use cases is evaluated.
The first section presents the datasets provided by MIWEnergia\footnote{ \url{https://www.miwenergia.com/} }.
Subsequently, the adapted evaluation methodology is described.
Finally, the evaluation of the performance of the models for the different use cases is presented.
After this chapter, it will be clear how the system has been validated and what the performance achieved by the proposed system is.


\section{MIWEnergia datasets}
\label{sec:datasets}
\vspace{0.2 cm}

In this section, the MIWEnergia datasets are described.
They provided 3 kinds of datasets: aggregated consumption data from all their customers, consumption data from single customers, and production data from PV plants.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/demand/data_plot}
\subcaption{}
\label{fig:demanddataplot}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/demand/customers_plot}
\subcaption{}
\label{fig:customersplot}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.5\textwidth]{images/demand/mean_data_plot}
\subcaption{}
\label{fig:meandemanddataplot}
\end{minipage}
\caption{The graphical representation of the hourly \subref{fig:demanddataplot} aggregated consumption over customers, \subref{fig:customersplot} number of customers, and \subref{fig:meandemanddataplot} average consumption per customer.}
\end{figure}

The aggregated consumption data from all their customers consists of hourly aggregated consumption data from June 2021 to January 2023 for a total of 14617 entries.
The graphical representation of the hourly aggregated consumption data is reported in figure~\ref{fig:demanddataplot}.
The number of customers is variable, with a maximum of 4253, a minimum of 1591, a mean of 2988, and a standard deviation of 855.
The graphical representation of the hourly number of customers is reported in figure~\ref{fig:customersplot}.
It was thought to normalize the consumption on the basis of the number of customers in order to study the average consumption per user, as illustrated in figure~\ref{fig:meandemanddataplot}, and then multiply by the number of customers.
However, this was not feasible since often this value changes significantly without reflecting on the consumption data, this information was deemed unreliable and not utilized.
Despite this limitation, the average consumption per customer still provided valuable insights into consumption patterns.
Specifically, it suggested the presence of two consumption peaks: one in the summer, likely attributed to air conditioning systems, and a second peak in the winter, likely caused by heating systems.

The consumption data from single customers consists of hourly aggregated consumption data of three customers:
\begin{enumerate}
  \item from June 2021 to August 2022 for a total of 10952 total entries and it is represented in figure~\ref{fig:dataplotcustomer1};
  \item from September 2021 to May 2022 for a total of 5855 total entries and it is represented in figure~\ref{fig:dataplotcustomer2};
  \item from September 2021 to August 2022 for a total of 8760 total entries and it is represented in figure~\ref{fig:dataplotcustomer3}.
\end{enumerate}
The total entries for the three customers are 25567 in the overall period from June 2021 to August 2022.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/data_plot_customer1}
\subcaption{First customer.}
\label{fig:dataplotcustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/data_plot_customer2}
\subcaption{Second customer.}
\label{fig:dataplotcustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.5\textwidth]{images/baseline/data_plot_customer3}
\subcaption{Third customer.}
\label{fig:dataplotcustomer3}
\end{minipage}
\caption{The graphical representation of the consumption data of the three customers.}
\end{figure}

Only consumption data for three customers were provided, as electricity consumption data of customers is considered personal data as stated in the Directive (EU) 2019/944 of the European Parliament\footnote{ \url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0944} }, which establishes common rules for the internal market for electricity.
As such, the data falls under the scope of the General Data Protection Regulation (GDPR)\footnote{ \url{https://gdpr-info.eu/} }, which requires explicit consent for processing personal data.
Therefore, it is likely that the provided data came from MIWEnergia's internal employees who consented to participate in the research.
In fact, the provided data were needed for a technical feasibility study.
However, to identify consumption patterns among a broader population, data from more customers would be required.

By analyzing the data plots, it can be observed that the consumption patterns of the three customers are quite distinct.
For instance:
\begin{itemize}
  \item The first customer's consumption is consistently below 1.5 kWh throughout the year except for the winter season;
  \item The second customer shows almost the same consumption pattern;
  \item The third customer has a dense series with some gaps with very low consumption in parts of the year, probably due to time periods away from home.
\end{itemize}

The production data from 8 PV plants consists of hourly aggregated production data:
\begin{enumerate}
  \item from January 2022 to October 2022 for a total of 7296 total entries and it has a nominal power of 149.75 kW;
  \item from February 2022 to October 2022 for a total of 6552 total entries and it has a nominal power of 237.6 kW;
  \item from February 2022 to October 2022 for a total of 6552 total entries and it has a nominal power of 158.4 kW;
  \item from June 2022 to October 2022 for a total of 3576 total entries and it has a nominal power of 1240 kW;
  \item from September 2022 to October 2022 for a total of 1465 total entries and it has a nominal power of 126.2 kW;
  \item from September 2022 to October 2022 for a total of 1465 total entries and it has a nominal power of 113 kW;
  \item from September 2022 to October 2022 for a total of 1465 total entries and it has a nominal power of 45 kW;
  \item from September 2022 to October 2022 for a total of 1465 total entries and it has a nominal power of 100 kW;
\end{enumerate}
The total entries for the 8 PV plants are 29836 total entries in the overall period from January 2022 to October 2022, aggregating over plants the resulting entries are 7296.
The graphical representation of the hourly aggregated total and mean percentage production data are reported respectively in figure~\ref{fig:productiondataplot} and figure~\ref{fig:productiondataplotpercentage}.

The same weather data from the same provider and the same weather station were used for all the tasks.
While it is reasonable to assume that some weather conditions, such as clouds, may vary and affect consumption and production differently, this approach was taken since the customers and the PV plants were in the same area, with a maximum distance of around 100 km between them, and also because the value of the weather parameters is an average of the values recorded in the past hour.
Therefore, using the same weather data is deemed appropriate for the analysis at hand, given that it reflects the weather conditions in the area over a broad time frame.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/production/data_plot}
\subcaption{}
\label{fig:productiondataplot}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/production/data_plot_percentage}
\subcaption{}
\label{fig:productiondataplotpercentage}
\end{minipage}
\caption{The graphical representation of the hourly aggregated \subref{fig:productiondataplot} total and \subref{fig:productiondataplotpercentage} mean percentage production data.}
\end{figure}


\section{Evaluation methodology}
\label{sec:methodology}
\vspace{0.2 cm}

The evaluation methodology was based on two relevant error metrics: Mean Absolute Percentage Error (MAPE) and Mean Absolute Error (MAE).
These are standard and widely used metrics in time series forecasting for different use cases, as reported in many articles and books such as \cite{armstrong2001principles, DEGOOIJER2006443, HYNDMAN2006679}.
The MAPE is defined as $\text{MAPE}(y, \hat{y}) = \frac{100\%}{N} \sum_{i=0}^{N - 1} \frac{|y_i - \hat{y}_i|}{|y_i|}$.
It is the most relevant error metric for all the tasks since it is a percentage-based error metric that takes into account the magnitude of the errors relative to the actual values.
The MAE is defined as $\text{MAE}(y, \hat{y}) = \frac{ \sum_{i=0}^{N - 1} |y_i - \hat{y}_i| }{N}$.
It is the most suitable error metric in consumption baseline forecasting where there is a high variability from very low to high values, and electricity production forecasting where there is a significant number of zeros when the sun is absent.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/cross_validation}
\caption{The schematic representation of the blocked k-fold cross-validation adopted.}
\label{fig:crossvalidation}
\end{figure}

For assessing the model performance when dealing with time series data, the traditional cross-validation techniques are not suitable as they assume that the data points are independent and identically distributed (i.i.d.), which is not the case in time series data, as also reported in the chapter~\ref{cha:soa} by the following articles \cite{BERGMEIR2012192, Cerqueira2020}.
In fact, in time series data the order of the observations matters, and there are temporal dependencies between the observations.
Therefore, a more appropriate technique for evaluating time series models is blocked k-fold cross-validation.
The basic idea of blocked k-fold cross-validation is to split the data considering multiple training and test sets, where the training set only includes data from the past and the test set includes data from the future.
This simulates the real-world scenario where we want to make predictions about the future based on past data.
In figure~\ref{fig:crossvalidation}, the schematic representation of how the blocked k-fold cross-validation was performed is reported, 12 splits were used with a test size depending on the specific use case.
The training set is increased by adding the previous test elements at every successive evaluation.

Another technique, reported in \cite{Cerqueira2020} is the repeated Holdout Out‐of‐sample tested in multiple testing periods with a Monte Carlo simulation using 70\% of the total observations of the time series in each test.
For each period, a random point is picked from the time series.
The previous window comprising 60\% of the time series is used for training and the following window of 10\% of the time series is used for testing.
In the paper, it was stated that the approach provided the most accurate estimates when the time series are non-stationary.
However, for having the same evaluation mechanisms on all the models and simulating the fact that the model starts with limited data and then the training amount increases over time to obtain better performance, block validation was used using the TimeSeriesSplit provided by the scikit-learn library.
Moreover, the considered data are without a strong trend as reported in the dedicated sections of the specific use cases where the data and the model forecasts are analyzed.

In addition to the blocked k-fold cross-validation results also the results on the last test split using the rest as training are reported.
This was done since it provides insight on which could be the performance of the models in forecasting the near future data with the currently available training data.
A table summarizing the metrics and the evaluation mechanisms adopted for the use cases is reported in table~\ref{tab:metricstable}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Use case} & \textbf{Metrics} & \textbf{Evaluation mechanisms}\\
\hline
Electricity demand & MAPE & blocked k-fold cross-validation\\
forecasting & & and test on the last split\\
\hline
Electricity production & MAE & blocked k-fold cross-validation\\
forecasting & & and test on the last split\\
\hline
Consumption baseline & MAPE and MAE & blocked k-fold cross-validation\\
forecasting & & and test on the last split\\
\hline
\end{tabular}
\caption{Table summarizing the metrics and the evaluation mechanisms adopted for the use cases.}
\label{tab:metricstable}
\end{table}

This methodology was used at training time to study the performance over time.
At forecasting time, the performance on the last time slot was treated with more attention as the most relevant for the energy retailers.
The combinations of the models were done just at forecasting time on the last time slot using the average of some approaches for studying whether there could be a beneficial effect on the forecasts and compensation of error between the different approaches.
The same was also done for the AutoML approach, in fact, it was tested just at forecasting time on the last time slot for comparison to the tested models.
It was included as a baseline to show how a general-purpose framework can perform in specific use cases compared to specific architectures designed for the task.

As explained in chapter~\ref{cha:implementation}, for all the use cases, there is a difference between training and forecasting time for the ML (support vector regressor, hist gradient boosting regressor, and extreme gradient boosting regressor) and DL (LSTM, GRU, and CNN) models.
In particular, for these models, a recursive strategy to generate multi-step forecasts was adopted.
This approach is a standard classical technique covered in various textbooks, including \cite{Manu2022}.
It offers advantages such as simplicity in understanding, clear temporal dependency as previous forecasts are used as input for subsequent ones, flexibility to be applied to different models, and adaptability to various forecasting horizons, which is crucial when integrating it into a SaaS solution.
However, there are also disadvantages associated with this approach.
Error propagation can occur, and this can be observed in the forecasting results, where small inaccuracies in initial predictions can accumulate and lead to larger errors over time.
Sub-optimality is another drawback, as the optimality achieved for one-step forecasting may not generalize well to multi-step forecasting.
Additionally, the computational complexity is high as the model needs to be run for each individual step.
In contrast, the other approaches: baselines, ARIMA/SARIMA, Prophet, and TFT directly handle the concept of multi-step prediction and present no difference between training and forecasting time.


\section{Electricity demand forecasting}
\label{sec:demandval}
\vspace{0.2 cm}

The aggregated consumption data over the customers is analyzed to get some descriptive analytics before finding adequate models to forecast the demand.
The time series decomposition using an additive model of the hourly aggregated consumption over the customers considering as period of the time series a week is reported in figure~\ref{fig:demanddecomposition}.
It showed a considerable amount of noise, comparable to seasonality in magnitude.
The trend showed 2 peaks during the summer season, the first being more emphasized since also corresponds to a peak in the number of users.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/demand/hourly_decomposition_week_period}
\caption{The time series decomposition of the hourly aggregated consumption over the customers considering as period of the time series a week.}
\label{fig:demanddecomposition}
\end{figure}

The auto-correlation of the hourly aggregated consumption over the customers is reported in figure~\ref{fig:demandcorrelation}.
It shows a high auto-correlation value in the close time lags and also at every 24 hours, along with an even higher value at a one-week distance.
This indicates that the consumption data from the closest time lags, particularly up to three closest ones, as well as those corresponding to the same hour in the preceding days and even better in the preceding weeks, may be valuable features for predicting a time instant's demand.
A reasonable balance can be achieved by incorporating the consumption data from the past 14 days.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/demand/hourly_correlation_week_range}
\caption{The auto-correlation of the hourly aggregated consumption over the customers.}
\label{fig:demandcorrelation}
\end{figure}

The coefficients given by the Fourier transform for the hourly aggregated consumption over the customers are reported in figure~\ref{fig:demandft}.
The graphical representation shows 2 main frequencies, one representing the weekly periodicity and one the daily periodicity.
Other minor peaks are present mostly at multiples of the 1/week frequency.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/demand/ft_hour_week}
\caption{The coefficients given by the Fourier transform for the hourly aggregated consumption over the customers.}
\label{fig:demandft}
\end{figure}

The daily aggregated consumption over the customers is reported in figure~\ref{fig:demanddataplotday}.
Aggregating the data on a daily basis, it is possible to more clearly observe the weekly pattern of consumption, which exhibits an increase on weekdays and a decrease on weekends.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/demand/data_day_aggregated_plot}
\caption{The daily aggregated consumption over the customers.}
\label{fig:demanddataplotday}
\end{figure}

Figure~\ref{fig:demanddecompositionday} shows the time series decomposition of the daily aggregated consumption over the customers, using an additive model with a period of one week.
The decomposition is consistent with the hourly aggregated consumption over the customers.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/demand/daily_aggregated_decomposition}
\caption{The time series decomposition of the daily aggregated consumption over the customers considering as period of the time series a week.}
\label{fig:demanddecompositionday}
\end{figure}

The auto-correlation of the daily aggregated consumption over the customers is reported in figure~\ref{fig:demandcorrelationday}.
It shows a high auto-correlation value in the closest time lag, with an even greater value at every week lag.
This suggests that useful features, since highly correlated, for predicting a time instant's demand may be the closest ones, but also those from the same day of the week in the previous weeks.
A good trade-off can be achieved by considering the consumption data in the previous 14 days.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/demand/daily_aggregated_correlation}
\caption{The auto-correlation of the daily aggregated consumption over the customers.}
\label{fig:demandcorrelationday}
\end{figure}

The coefficients given by the Fourier transform for the daily aggregated consumption over the customers are reported in figure~\ref{fig:demandftday}.
The graphical representation shows 1 main frequency representing the weekly periodicity and other minor peaks at multiples of the 1/week frequency.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/demand/ft_day}
\caption{The coefficients given by the Fourier transform for the daily aggregated consumption over the customers.}
\label{fig:demandftday}
\end{figure}

Basic data is enhanced with the air temperature, the apparent temperature, and the relative humidity since they are considered the only weather features capable of influencing customers' energy consumption.
To assess the relationship between these weather variables and the aggregated consumption over the customers, two correlation coefficients were used: Pearson's correlation coefficient and Spearman's rank correlation coefficient.
Pearson's correlation coefficient measures the strength of the linear relationship between two variables, while Spearman's rank correlation coefficient measures the strength of the monotonic relationship.
The pearsonr and spearmanr methods of the SciPy library\footnote{ \url{https://scipy.org/} } were used to compute the correlation with weather data.
The results showed that the hourly aggregated consumption over the customers had:
\begin{itemize}
  \item a Pearson correlation coefficient of 0.4480 and a Spearman's rank correlation coefficient of 0.4378 with respect to the air temperature;
  \item a Pearson correlation coefficient of 0.4475 and a Spearman's rank correlation coefficient of 0.4368 with respect to the apparent temperature;
  \item a Pearson correlation coefficient of -0.2689 and a Spearman's rank correlation coefficient of -0.3173 with respect to the relative humidity.
\end{itemize}
It can be noticed that both the coefficients indicate a moderate correlation between the weather variables and consumption, this suggests that incorporating weather data into the prediction models could be useful.

The results showed that the daily aggregated consumption over the customers had:
\begin{itemize}
  \item a Pearson correlation coefficient of 0.3898 and a Spearman's rank correlation coefficient of 0.3119 with respect to the air temperature;
  \item a Pearson correlation coefficient of 0.3823 and a Spearman's rank correlation coefficient of 0.3114 with respect to the apparent temperature;
  \item a Pearson correlation coefficient of -0.0908 and a Spearman's rank correlation coefficient of -0.1547 with respect to the relative humidity.
\end{itemize}
It can be noticed that both the coefficients decreased considering the daily data meaning that the mean weather data over the day is less correlated to the daily aggregated consumption over the customers, compared to the hourly granularity.

After this data analysis, the specific parameters used in the models can be explained more in detail.
The baseline approaches are built considering the repetition of past days and weeks since data presents a high correlation with that time instants and could lead to a reasonable baseline performance to achieve.
The SARIMA model considers the week as the period for seasonal differencing since data presents a high correlation with that time instants and the model can try to take advantage of the weekly seasonality.
The support vector regressor model uses a radial basis function kernel with a configuration of the C parameter (regularization parameter of squared l2 penalty) to 1.0 to penalize the complexity of the model, and the epsilon parameter defining the epsilon-tube for no penalty to 0.1 as a trade-off between accuracy and generalization.
The hist gradient boosting regressor model uses as loss the absolute error since this was the supported metric closer to the objective and keeping the default parameters for tree definition.
The same was also done for the extreme gradient boosting regressor model.
The Prophet model was built keeping the default parameters for automatically detecting seasonalities and best fitting the training data.

The LSTM model was designed as a 2-layer model with 24 Bidirectional LSTM units in the first layer which use rectified linear unit (ReLU) activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
Batch normalization is then applied before entering the second layer composed of 16 Bidirectional LSTM units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using mean absolute percentage error as loss and Nadam (a version of Adam integrating the concept of Nesterov momentum) as optimizer with a learning rate of 0.005.

The GRU model was designed as a 3-layer model with 24 Bidirectional GRU units in the first layer which use ReLU activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
Subsequently, the second layer is composed of 16 Bidirectional GRU units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the second layer goes inside a third dense layer composed of 4 units with ReLU activation function before entering the final dense unit to output the final prediction.
The model is trained using mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The CNN model was designed as a 3-layer model with a first layer composed of 24 1D Convolutional units with a kernel size of 5 which uses ReLU activation function.
A 1D max polling operation is applied before entering the second layer composed of 16 1D Convolutional units with a kernel size of 3 which use ReLU activation function.
A 1D max polling operation is applied before the flatting operation and entering the third dense layer composed of 4 units with ReLU activation function.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The TFT model was configured with 2 LSTM layers with 16 as hidden size, 4 as attention size, and 8 as hidden continuous size.
The dropout is set to 0.02.
The model is trained using mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

% TODO: Giustificare un po’ di più i parametri --> provati e verificato che avessero buone performance (provando x valori questo andava meglio)

12 splits were used for block validation with a test size of a month each time, namely 30 days of data.
For the hourly granularity, the models start with 5977 entries as training and predict every time 720 entries until reaching the last prediction instant where the model has a training size of 13897.
The results for hourly aggregation are reported in table~\ref{tab:demandhourlyresults}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
 & \textbf{cross-validation MAPE} & \textbf{MAPE}\\
\hline
TFT & 15.00 $\pm$ 4.31 & 14.76\\
\hline
One Week Baseline & 12.94 $\pm$ 4.27 & 17.55\\
\hline
SARIMA & 15.44 $\pm$ 4.57 & 18.47\\
\hline
One Day Baseline & 22.86 $\pm$ 7.07 & 20.54\\
\hline
GRU & 30.77 $\pm$ 14.48 & 22.83\\
\hline
XGBRegressor & 23.84 $\pm$ 7.18 & 25.92\\
\hline
HistGradientBoostingRegressor & 19.43 $\pm$ 6.35 & 27.36\\
\hline
CNN & 34.64 $\pm$ 15.76 & 28.83\\
\hline
SVR & 61.77 $\pm$ 22.61 & 29.43\\
\hline
Prophet & 28.70 $\pm$ 7.10 & 31.66\\
\hline
LSTM & 34.08 $\pm$ 11.92 & 32.51\\
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation.}
\label{tab:demandhourlyresults}
\end{table}

% TODO sortare per mape e capire quele dei 2, + spiegare differenze ranking, deep dive su lstm o altri modelli with differenze rilevanti

The results for hourly aggregation at forecasting time on the last time slot including also the combinations of different tecnhiques and the AutoML approach are reported in table~\ref{tab:demandhourlyresultsforecast}.
The combination of the previous techniques was performed by combining one-day baseline, one-week baseline, and prophet with possibly one of LSTM, GRU, or CNN with the intent of trying to improve the robustness of DL models with some stable baselines and prophet model which internally automatically deals with seasonalities.
The AutoML approach uses the mean MAPE forecasting as loss and it is launched for 10 hours with a maximum function evaluation time of 2 hours.
Then the best ensemble of the found models is returned and can be used to forecast.
In the table is also reported the performance on the one week horizon, namely 7 days of data, from which it is possible to gain more insights on the models forecasts.

% TODO ha senso ottimizzare la combinazione (a training e poi vedere a forecasting), maybe LR on the models on a validation set instead of just the mean

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Forecast on the last split} & \textbf{Forecast on the last split}\\
 & \textbf{one-week MAPE} & \textbf{one-month MAPE}\\
\hline
LSTM + One Week Baseline + & 15.35 & 13.97\\
Prophet & & \\
\hline
TFT & 13.71 & 14.76\\
\hline
LSTM + One Day Baseline + & 13.70 & 14.91\\
One Week Baseline + Prophet & & \\
\hline
CNN + One Week Baseline + & 14.17 & 15.04\\
Prophet & & \\
\hline
CNN + One Day Baseline + & 13.00 & 15.87\\
One Week Baseline + Prophet & & \\
\hline
AutoML & 14.68 & 16.96\\
\hline
One Week Baseline & 10.87 & 17.55\\
\hline
CNN + One Week Baseline & 19.00 & 18.43\\
\hline
SARIMA & 12.28 & 18.47\\
\hline
LSTM + One Week Baseline & 20.76 & 19.22\\
\hline
GRU + One Week Baseline & 10.95 & 19.67\\
\hline
One Day Baseline & 14.78 & 20.54\\
\hline
One Day Baseline + & 15.16 & 20.99\\
One Week Baseline + Prophet & & \\
\hline
GRU + One Day Baseline + & 14.10 & 21.19\\
One Week Baseline + Prophet & & \\
\hline
GRU + One Week Baseline + & 15.93 & 22.38\\
Prophet & & \\
\hline
GRU & 13.43 & 22.83\\
\hline
One Week Baseline + Prophet & 19.03 & 22.83\\
\hline
XGBRegressor & 15.59 & 25.92\\
\hline
HistGradientBoostingRegressor & 14.02 & 27.36\\
\hline
CNN & 33.22 & 28.83\\
\hline
SVR & 29.02 & 29.43\\
\hline
Prophet & 33.95 & 31.66\\
\hline
LSTM & 37.67 & 32.51\\
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation at forecasting time.}
\label{tab:demandhourlyresultsforecast}
\end{table}

Analyze the forecasts of the models for the electricity demand forecasting task (predictive analytics) ...

% TODO perche alcuni modelli di ml vanno meglio di modelli di dl più sofisticati, magari con meno dati rete piu complessa fa fatica
% Modelli in letteratura che vanno super bene vado peggio di baseline per esempio
% Non necessariamente ML va meglio --> training magari non ottimale. dipende molto dai dati

% TODO mettere grafici di forecast di modelli che vanno bene + scatterplot (per mostrare no bias) 

% TODO Descrivere un po’ i risultati (magari con qualche grafico)  --> tirare fuori insight, cosa capisco, cosa si nota, ...

% Discussione su AutoML dopo risultati, parlando di maturità e problematiche, topic molto hot dato che molto potente potenzialmente (ha molto potenziale)

For the daily granularity, the models start with 249 entries as training and predict every time 30 entries until reaching the last prediction instant where the model has a training size of 579.
The results for daily aggregation are reported in table~\ref{tab:demanddailyresults}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
 & \textbf{cross-validation MAPE} & \textbf{MAPE}\\
\hline
XGBRegressor & 13.06 $\pm$ 5.43 & 10.49\\
\hline
CNN & 50.55 $\pm$ 36.81 & 10.55\\
\hline
HistGradientBoostingRegressor & 12.41 $\pm$ 5.18 & 12.19\\
\hline
LSTM & 29.60 $\pm$ 13.81 & 14.51\\
\hline
TFT & 13.73 $\pm$ 5.06 & 14.88\\
\hline
One Week Baseline & 11.50 $\pm$ 4.78 & 18.40\\
\hline
SARIMA & 11.92 $\pm$ 7.04 & 19.63\\
\hline
GRU & 28.76 $\pm$ 29.26 & 20.47\\
\hline
One Day Baseline & 22.80 $\pm$ 8.21 & 21.51\\
\hline
Prophet & 19.10 $\pm$ 10.74 & 26.34\\
\hline
SVR & 53.59 $\pm$ 22.35 & 39.85\\
\hline
\end{tabular}
\caption{Table summarizing the results for daily aggregation.}
\label{tab:demanddailyresults}
\end{table}

The results for daily aggregation at forecasting time on the last time slot including also the combinations of different tecnhiques and the AutoML approach are reported in table~\ref{tab:demanddailyresultsforecast}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Forecast on the last split} & \textbf{Forecast on the last split}\\
 & \textbf{one-week MAPE} & \textbf{one-month MAPE}\\
\hline
XGBRegressor & 14.00 & 10.49\\
\hline
HistGradientBoostingRegressor & 11.60 & 12.19\\
\hline
LSTM + One Week Baseline & 11.27 & 12.93\\
\hline
CNN + One Week Baseline & 10.67 & 14.37\\
\hline
LSTM & 18.40 & 14.55\\
\hline
TFT & 12.65 & 14.88\\
\hline
CNN & 18.71 & 15.15\\
\hline
LSTM + One Week Baseline + & 8.82 & 15.91\\
Prophet & & \\
\hline
GRU + One Week Baseline & 10.93 & 16.55\\
\hline
LSTM + One Day Baseline + & 9.85 & 17.29\\
One Week Baseline + Prophet & & \\
\hline
CNN + One Week Baseline + & 9.02 & 17.38\\
Prophet & & \\
\hline
One Week Baseline & 9.63 & 18.40\\
\hline
CNN + One Day Baseline + & 10.01 & 18.40\\
One Week Baseline + Prophet & & \\
\hline
GRU + One Week Baseline + & 8.27 & 18.42\\
Prophet & & \\
\hline
GRU & 17.69 & 18.80\\
\hline
GRU + One Day Baseline + & 9.45 & 19.18\\
One Week Baseline + Prophet & & \\
\hline
SARIMA & 11.23 & 19.63\\
\hline
AutoML & 10.61 & 20.98\\
\hline
One Day Baseline & 13.14 & 21.51\\
\hline
One Day Baseline + & 11.73 & 21.69\\
One Week Baseline + Prophet & & \\
\hline
One Week Baseline + Prophet & 12.83 & 22.21\\
\hline
Prophet & 17.36 & 26.34\\
\hline
SVR & 58.08 & 39.81\\
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation at forecasting time.}
\label{tab:demanddailyresultsforecast}
\end{table}

% TODO check the consistency of model results in the last period and using cross-validation --> do a table with the differences (+ plot for internal analysis)
% TODO then add the comparison with forecasts of their provider
% commentarli (sia il numero che il grafico dell’errore) --> interpretare il dato (perchè è buono?) --> benchmark in letteratura, come va il loro, ecc…
% [valutare criticamente, aspetto tecnologico scientifico]


\section{Electricity production forecasting}
\label{sec:productionval}
\vspace{0.2 cm}

As described in the data preprocessing in chapter~\ref{cha:implementation}, single PV plant production data are aggregated to obtain the aggregated production data over the PV plants.
The target of the predictions is the mean percentage of production, which is calculated as the division of the total produced energy by the total power of the PV plants.
This allows us to have a bounded value from 0 to 100 from which it is possible to obtain the total produced energy simply by multiplying it by the total power of the PV plants.
This was also done since PV plants are added over time and this was unpredictable, this results in predicting the percentage of production of the PV plants.
This data is analyzed to get some descriptive analytics before finding adequate models to forecast the production.
The time series decomposition using an additive model of the hourly percentage of production considering as period of the time series a day is reported in figure~\ref{fig:productiondecomposition}.
It showed a considerable amount of noise, comparable to seasonality and trend in magnitude.
The trend component exhibits a clear peak during the summer season.
As expected, the seasonality component appears to have the most significant impact on the time series.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/production/hourly_decomposition}
\caption{The time series decomposition of the hourly percentage of production considering as period of the time series a day.}
\label{fig:productiondecomposition}
\end{figure}

The auto-correlation of the hourly percentage of production is reported in figure~\ref{fig:productioncorrelation}.
It shows a high auto-correlation value in the closest time lag and also at every 24 hours, with the value slightly decreasing as the lag increases over days.
This indicates that the production data from the closest time lag and those corresponding to the same hour in the preceding days may be valuable features for predicting a time instant's production.
A reasonable balance can be achieved by incorporating the production data from the past 14 days.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/production/hourly_correlation_week_range}
\caption{The auto-correlation of the hourly percentage of production.}
\label{fig:productioncorrelation}
\end{figure}

The coefficients given by the Fourier transform for the hourly percentage of production are reported in figure~\ref{fig:productionft}.
The graphical representation clearly shows a main frequency at the daily periodicity.
Other minor peaks are present mostly at multiples of the 1/day frequency.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/production/ft_hour_day}
\caption{The coefficients given by the Fourier transform for the hourly percentage of production.}
\label{fig:productionft}
\end{figure}

The daily percentage of production is reported in figure~\ref{fig:productiondataplotday}.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/production/data_day_aggregated_plot}
\caption{The daily percentage of production.}
\label{fig:productiondataplotday}
\end{figure}

The time series decomposition using an additive model of the daily percentage of production considering as period of the time series a week is reported in figure~\ref{fig:productiondecompositionday}.
It showed a considerable amount of noise, comparable to the trend in magnitude when not in the summer season.
The trend component exhibits a clear peak during the summer season and appears to be the significant component of the time series.
In this case, considering a weekly period, the seasonality component appears to not have a significant impact on the time series.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/production/daily_aggregated_decomposition}
\caption{The time series decomposition of the daily percentage of production considering as period of the time series a week.}
\label{fig:productiondecompositionday}
\end{figure}

The auto-correlation of the daily percentage of production is reported in figure~\ref{fig:productioncorrelationday}.
It shows a high auto-correlation value in the closest time lag and then a slight decrease as the lag increases over days.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/production/daily_aggregated_correlation}
\caption{The auto-correlation of the daily percentage of production.}
\label{fig:productioncorrelationday}
\end{figure}

The coefficients given by the Fourier transform for the daily percentage of production are reported in figure~\ref{fig:productionftday}.
As expected from the time series decomposition, the graphical representation exhibits that there are no main frequencies in the daily percentage of production.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/production/ft_day}
\caption{The coefficients given by the Fourier transform for the daily percentage of production.}
\label{fig:productionftday}
\end{figure}

Basic data is enhanced with the air temperature, the apparent temperature, the relative humidity, the wind speed, the wind direction, the pressure altimeter, the visibility, the sky coverage, the diffuse horizontal irradiance, the direct normal irradiance, the global horizontal irradiance, the solar radiation, the UV index, the solar elevation angle, and the solar azimuth angle.
To assess the relationship between these weather variables and the percentage of production, two correlation coefficients were used: Pearson's correlation coefficient and Spearman's rank correlation coefficient.
Pearson's correlation coefficient measures the strength of the linear relationship between two variables, while Spearman's rank correlation coefficient measures the strength of the monotonic relationship.
The pearsonr and spearmanr methods of the SciPy library were used to compute the correlation with weather data.
The results showed that the hourly percentage of production had:
\begin{itemize}
  \item a Pearson correlation coefficient of 0.5621 and a Spearman's rank correlation coefficient of 0.5185 with respect to the air temperature;
  \item a Pearson correlation coefficient of 0.5410 and a Spearman's rank correlation coefficient of 0.5032 with respect to the apparent temperature;
  \item a Pearson correlation coefficient of -0.6318 and a Spearman's rank correlation coefficient of -0.5969 with respect to the relative humidity;
  \item a Pearson correlation coefficient of 0.3105 and a Spearman's rank correlation coefficient of 0.3237 with respect to the wind speed;
  \item a Pearson correlation coefficient of -0.1919 and a Spearman's rank correlation coefficient of -0.2155 with respect to the wind direction;
  \item a Pearson correlation coefficient of -0.2092 and a Spearman's rank correlation coefficient of -0.2645 with respect to the pressure altimeter;
  \item a Pearson correlation coefficient of -0.0132 and a Spearman's rank correlation coefficient of 0.0355 with respect to the visibility;
  \item a Pearson correlation coefficient of -0.2822 and a Spearman's rank correlation coefficient of -0.2375 with respect to the sky coverage;
  \item a Pearson correlation coefficient of 0.8559 and a Spearman's rank correlation coefficient of 0.9340 with respect to the diffuse horizontal irradiance;
  \item a Pearson correlation coefficient of 0.8399 and a Spearman's rank correlation coefficient of 0.9315 with respect to the direct normal irradiance;
  \item a Pearson correlation coefficient of 0.8791 and a Spearman's rank correlation coefficient of 0.9357 with respect to the global horizontal irradiance;
  \item a Pearson correlation coefficient of 0.9073 and a Spearman's rank correlation coefficient of 0.9472 with respect to the solar radiation;
  \item a Pearson correlation coefficient of 0.8571 and a Spearman's rank correlation coefficient of 0.9392 with respect to the UV index;
  \item a Pearson correlation coefficient of 0.8107 and a Spearman's rank correlation coefficient of 0.9057 with respect to the solar elevation angle;
  \item a Pearson correlation coefficient of 0.0450 and a Spearman's rank correlation coefficient of 0.0465 with respect to the solar azimuth angle.
\end{itemize}
It can be noticed that both the coefficients indicate a moderate to strong correlation between the hourly percentage of production and several weather features, including the air temperature, the apparent temperature, the relative humidity, the diffuse horizontal irradiance, the direct normal irradiance, the global horizontal irradiance, the solar radiation, the UV index, and the solar elevation angle.
Therefore, these features are chosen to be incorporated into the prediction models, while the other weather features show weaker correlations and are not considered.

The results showed that the daily percentage of production had:
\begin{itemize}
  \item a Pearson correlation coefficient of 0.7632 and a Spearman's rank correlation coefficient of 0.7721 with respect to the air temperature;
  \item a Pearson correlation coefficient of 0.7434 and a Spearman's rank correlation coefficient of 0.7471 with respect to the apparent temperature;
  \item a Pearson correlation coefficient of -0.7598 and a Spearman's rank correlation coefficient of -0.7474 with respect to the relative humidity;
  \item a Pearson correlation coefficient of 0.1773 and a Spearman's rank correlation coefficient of 0.3769 with respect to the wind speed;
  \item a Pearson correlation coefficient of -0.2405 and a Spearman's rank correlation coefficient of -0.2689 with respect to the wind direction;
  \item a Pearson correlation coefficient of -0.2506 and a Spearman's rank correlation coefficient of -0.2559 with respect to the pressure altimeter;
  \item a Pearson correlation coefficient of 0.2545 and a Spearman's rank correlation coefficient of 0.3259 with respect to the visibility;
  \item a Pearson correlation coefficient of -0.7004 and a Spearman's rank correlation coefficient of -0.6542 with respect to the sky coverage;
  \item a Pearson correlation coefficient of 0.6244 and a Spearman's rank correlation coefficient of 0.6939 with respect to the diffuse horizontal irradiance;
  \item a Pearson correlation coefficient of 0.6216 and a Spearman's rank correlation coefficient of 0.6923 with respect to the direct normal irradiance;
  \item a Pearson correlation coefficient of 0.6305 and a Spearman's rank correlation coefficient of 0.6988 with respect to the global horizontal irradiance;
  \item a Pearson correlation coefficient of 0.7878 and a Spearman's rank correlation coefficient of 0.8013 with respect to the solar radiation;
  \item a Pearson correlation coefficient of 0.8389 and a Spearman's rank correlation coefficient of 0.8680 with respect to the UV index;
  \item a Pearson correlation coefficient of 0.6502 and a Spearman's rank correlation coefficient of 0.7079 with respect to the solar elevation angle;
  \item a Pearson correlation coefficient of 0.1457 and a Spearman's rank correlation coefficient of 0.2813 with respect to the solar azimuth angle.
\end{itemize}
It can be noticed that both the coefficients decreased meaning that the mean weather data over the day is less correlated to the daily percentage of production, compared to the hourly granularity.
Only the hourly granularity is considered since PV plants are highly correlated with weather data and the aggregation over the day loses part of this correlation.

If we were forecasting the production of each PV plant individually, the weather data at the specific locations of the plants would result in a higher correlation with the production data and likely allow the models to produce more accurate forecasts.
However, since we are aggregating the production from multiple PV plants and looking for the overall percentage of production in the hour, using the weather data from a location located in the middle of the PV plants is also acceptable.
In figure~\ref{fig:pvplantsmap}, the locations of the PV plants and of the weather station are reported.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/production/pv_plants_map}
\caption{The map of the city of Murcia with indicated the location of the PV plants with a blue icon and of the revelation station with a lightblue icon. The distances between each PV plant and the weather station are reported in lightblue boxes near the PV plant locations.}
\label{fig:pvplantsmap}
\end{figure}

After this data analysis, the specific parameters used in the models can be explained more in detail.
The baseline approach is built considering the repetition of past days since data presents a high correlation with that time instant and could lead to a reasonable baseline performance to achieve.
A simple ARIMA model considers the single hours without seasonally on the week period like for demand prediction since data does not present a relevant correlation with that time instants.
The support vector regressor model uses a radial basis function kernel with a configuration of the C parameter (regularization parameter of squared l2 penalty) to 1.0 to penalize the complexity of the model, and the epsilon parameter defining the epsilon-tube for no penalty to 0.1 as a trade-off between accuracy and generalization.
The hist gradient boosting regressor model uses as loss the absolute error since this was the supported metric closer to the objective and keeping the default parameters for tree definition.
The same was also done for the extreme gradient boosting regressor model.
The Prophet model was built keeping the default parameters for automatically detecting seasonalities and best fitting the training data.

The LSTM model was designed as a 2-layer model with 32 Bidirectional LSTM units in the first layer which use ReLU activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
The second layer is composed of 16 Bidirectional LSTM units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using mean absolute error as loss and Nadam as optimizer with a learning rate of 0.005.

The GRU model was designed as a 2-layer model with 32 Bidirectional GRU units in the first layer which use ReLU activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
Subsequently, the second layer is composed of 16 Bidirectional GRU units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using mean absolute error as loss and Nadam as optimizer with a learning rate of 0.005.

The CNN model was designed as a 2-layer model with a first layer composed of 24 1D Convolutional units with a kernel size of 5 which uses ReLU activation function.
A 1D max polling operation is applied before entering the second layer composed of 16 1D Convolutional units with a kernel size of 3 which use ReLU activation function.
A 1D max polling operation is applied before the flatting operation and entering the final dense unit to output the final prediction.
The model is trained using mean absolute error as loss and Nadam as optimizer with a learning rate of 0.005.

The combination of the previous techniques was performed by combining one-day baseline and prophet with possibly one of LSTM, GRU, or CNN with the intent of trying to improve the robustness of DL models with some stable baselines and prophet model which internally automatically deals with seasonalities.

The TFT model was configured with 2 LSTM layers with 16 as hidden size, 4 as attention size, and 8 as hidden continuous size.
The dropout is set to 0.02.
The model is trained using mean absolute error as loss and Nadam as optimizer with a learning rate of 0.005.

The AutoML approach uses the mean MAPE forecasting as loss and it is launched for 10 hours with a maximum function evaluation time of 2 hours.
Then the best ensemble of the found models is returned and can be used to forecast.

12 splits were used for block validation with a test size of a week each time, namely 7 days of data.
The models start with 5280 entries as training and predict every time 168 entries until reaching the last prediction instant where the model has a training size of 7128.
The results for hourly aggregation are reported in table~\ref{tab:productionhourlyresults}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
 & \textbf{cross-validation MAE percentage} & \textbf{MAE percentage}\\
\hline
HistGradientBoostingRegressor & 2.36 $\pm$ 0.55 & 1.91\\
\hline
GRU & 3.34 $\pm$ 0.83 & 2.06\\
\hline
XGBRegressor & 2.60 $\pm$ 0.45 & 2.07\\
\hline
CNN & 4.78 $\pm$ 1.20 & 2.86\\
\hline
SVR & 5.01 $\pm$ 1.06 & 3.02\\
\hline
LSTM & 5.38 $\pm$ 1.59 & 3.10\\
\hline
ARIMA & 5.09 $\pm$ 1.35 & 3.64\\
\hline
TFT & 4.60 $\pm$ 1.14 & 3.85\\
\hline
Prophet & 7.90 $\pm$ 1.62 & 4.38\\
\hline
One Day Baseline & 6.81 $\pm$ 3.14 & 4.58\\
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation.}
\label{tab:productionhourlyresults}
\end{table}

% Present before results at training time and then the ones at forecasting time?

The results for hourly aggregation at forecasting time are reported in table~\ref{tab:productionhourlyresultsforecast}.
% in this case no cross-validation, just on the last split + ensembles considered

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{Forecast on the last split}\\
 & \textbf{MAE percentage}\\
\hline
HistGradientBoostingRegressor & 2.86\\
\hline
GRU & 3.03\\
\hline
CNN & 3.08\\
\hline
SVR & 3.12\\
\hline
CNN + One Day Baseline & 3.25\\
\hline
XGBRegressor & 3.35\\
\hline
CNN + One Day Baseline + Prophet & 3.47\\
\hline
LSTM + One Day Baseline & 3.61\\
\hline
GRU + One Day Baseline & 3.62\\
\hline
ARIMA & 3.64\\
\hline
LSTM & 3.66\\
\hline
LSTM + One Day Baseline + Prophet & 3.82\\
\hline
GRU + One Day Baseline + Prophet & 3.83\\
\hline
TFT & 3.85\\
\hline
Prophet & 4.38\\
\hline
One Day Baseline + Prophet & 4.38\\
\hline
One Day Baseline & 4.58\\
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation at forecasting time.}
\label{tab:productionhourlyresultsforecast}
\end{table}

Instead, the AutoML approach obtained a MAE percentage of 5.89 after a total run of 10 hours.

Analyze the forecasts of the models for the electricity production forecasting task (predictive analytics) ...

% TODO check the consistency of model results in the last period and using cross-validation --> do a table with the differences (+ plot for internal analysis)
% TODO then add the comparison with forecasts of their provider
% commentarli (sia il numero che il grafico dell’errore) --> interpretare il dato (perchè è buono?) --> benchmark in letteratura, come va il loro, ecc…
% [valutare criticamente, aspetto tecnologico scientifico]


\section{Consumption baseline forecasting} 
\label{sec:baselineval}
\vspace{0.2 cm}

The consumption data of the three customers are analyzed to get some descriptive analytics before finding adequate models to forecast the consumption baseline of each customer.
The time series decompositions using an additive model of the hourly consumption of the three customers considering as period of the time series a week are reported in figure~\ref{fig:decompositioncustomer1}, figure~\ref{fig:decompositioncustomer2}, and figure~\ref{fig:decompositioncustomer3}.
All three decompositions show a considerable amount of noise, which is dominant in magnitude compared to the trend and seasonality.
For all the customers the seasonal component appears to not have a significant impact on the time series, with the exception of the second customer for which it is twice high in magnitude with respect to the others.
The trend component has a slight impact, for the first customer it is possible to see a peak in the winter, for the second customer it is present a peak in April, and for the third customer there is not a clear trend.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/hourly_decomposition_week_period_customer1}
\subcaption{First customer.}
\label{fig:decompositioncustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/hourly_decomposition_week_period_customer2}
\subcaption{Second customer.}
\label{fig:decompositioncustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.5\textwidth]{images/baseline/hourly_decomposition_week_period_customer3}
\subcaption{Third customer.}
\label{fig:decompositioncustomer3}
\end{minipage}
\caption{The time series decompositions of the hourly consumption of the three customers considering as period of the time series a week.}
\end{figure}

The auto-correlations of the hourly consumption of the three customers are reported in figure~\ref{fig:correlationcustomer1}, figure~\ref{fig:correlationcustomer2}, and figure~\ref{fig:correlationcustomer3}.
All the customers’ consumptions are not very auto-correlated, it can be shown that the maximum auto-correlation value is around 0.5 in the closest time lag and that there is a peak every 24 hours, with a slightly greater value at one week distance.
This indicates that the consumption data from the closest time lag, as well as those corresponding to the same hour in the preceding days and even better in the preceding weeks, may be valuable features for predicting a time instant's demand.
A reasonable balance can be achieved by incorporating the consumption data from the past 14 days.

The coefficients given by the Fourier transform for the hourly consumption of the three customers are reported in figure~\ref{fig:ftcustomer1}, figure~\ref{fig:ftcustomer2}, and figure~\ref{fig:ftcustomer3}.
The graphical representations show for all the customers a main frequency at the daily periodicity.
Other minor peaks are present mostly at multiples of the 1/day frequency, in particular for the third customer where the 2/day frequency is almost equal to the 1/day.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/hourly_correlation_week_range_customer1}
\subcaption{First customer.}
\label{fig:correlationcustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/hourly_correlation_week_range_customer2}
\subcaption{Second customer.}
\label{fig:correlationcustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.4\textwidth]{images/baseline/hourly_correlation_week_range_customer3}
\subcaption{Third customer.}
\label{fig:correlationcustomer3}
\end{minipage}
\caption{The auto-correlations of the hourly consumption of the three customers.}
\end{figure}

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/ft_hour_week_customer1}
\subcaption{First customer.}
\label{fig:ftcustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/ft_hour_week_customer2}
\subcaption{Second customer.}
\label{fig:ftcustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.4\textwidth]{images/baseline/ft_hour_week_customer3}
\subcaption{Third customer.}
\label{fig:ftcustomer3}
\end{minipage}
\caption{The coefficients given by the Fourier transform of the hourly consumption of the three customers.}
\end{figure}

The daily consumption of the three customers are reported in figure~\ref{fig:dataplotdaycustomer1}, figure~\ref{fig:dataplotdaycustomer2}, and figure~\ref{fig:dataplotdaycustomer3}.

The time series decompositions using an additive model of the daily consumption of the three customers considering as period of the time series a week are reported in figure~\ref{fig:decompositiondaycustomer1}, figure~\ref{fig:decompositiondaycustomer2}, and figure~\ref{fig:decompositiondaycustomer3}.
The decomposition is consistent with the hourly aggregated consumption over the customers.

The auto-correlations of the daily consumption of the three customers are reported in figure~\ref{fig:correlationdaycustomer1},  figure~\ref{fig:correlationdaycustomer2}, and  figure~\ref{fig:correlationdaycustomer3}.
With the daily aggregation, all the customers’ consumptions show are slightly high value with respect to the hourly granularity.
It can be shown that for the first and third customers, the auto-correlation value is slightly high in the closest time lag and then decreases with the days, with a slightly greater value at one week distance.
Instead, for the second customer the only relevant values are only at a multiple of 7 days.

The coefficients given by the Fourier transform for the daily consumption of the three customers are reported in figure~\ref{fig:ftdaycustomer1}, figure~\ref{fig:ftdaycustomer2}, and figure~\ref{fig:ftdaycustomer3}.
The graphical representation exhibits that there are no main frequencies in the daily consumption of the first and third customers.
Instead, for the second customer there is a small peak at the daily periodicity.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/data_day_aggregated_plot_customer1}
\subcaption{First customer.}
\label{fig:dataplotdaycustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/data_day_aggregated_plot_customer2}
\subcaption{Second customer.}
\label{fig:dataplotdaycustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.5\textwidth]{images/baseline/data_day_aggregated_plot_customer3}
\subcaption{Third customer.}
\label{fig:dataplotdaycustomer3}
\end{minipage}
\caption{The daily consumption of the three customers.}
\end{figure}

As can be noticed from the data, there is high variability in consumption and low auto-correlation, this suggests how it is difficult to produce highly accurate results on a single customer level.
With just the time series of a few users, it is very difficult to learn a well-performing model, having more users it could be possible to learn certain generic trends or standard behaviors.

Basic data is enhanced with the air temperature, the apparent temperature, and the relative humidity since they are considered the only weather features capable of influencing customers' energy consumption.
To assess the relationship between these weather variables and the consumption of the three customers, two correlation coefficients were used: Pearson's correlation coefficient and Spearman's rank correlation coefficient.
Pearson's correlation coefficient measures the strength of the linear relationship between two variables, while Spearman's rank correlation coefficient measures the strength of the monotonic relationship.
The pearsonr and spearmanr methods of the SciPy library were used to compute the correlation with weather data.
The results showed that the hourly consumption of the three customers had:
\begin{itemize}
  \item a Pearson correlation coefficient of -0.2491, 0.0240, and -0.1338 respectively and a Spearman's rank correlation coefficient of -0.2088, 0.1304, and -0.0196 respectively with respect to the air temperature;
  \item a Pearson correlation coefficient of -0.2434, 0.0266, and -0.1356 respectively and a Spearman's rank correlation coefficient of -0.2082, 0.1305, and -0.0207 respectively with respect to the apparent temperature;
  \item a Pearson correlation coefficient of 0.1193, -0.0845, and -0.0985 respectively and a Spearman's rank correlation coefficient of 0.1491, -0.1048, and -0.1123 respectively with respect to the relative humidity.
\end{itemize}
It can be noticed that both the coefficients indicate a weak correlation between the weather variables and the consumption of the three customers, nevertheless, it may still be useful to incorporate weather data into the prediction models since also the auto-correlation values are not particularly high.

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/daily_aggregated_decomposition_customer1}
\subcaption{First customer.}
\label{fig:decompositiondaycustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=1\textwidth]{images/baseline/daily_aggregated_decomposition_customer2}
\subcaption{Second customer.}
\label{fig:decompositiondaycustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.5\textwidth]{images/baseline/daily_aggregated_decomposition_customer3}
\subcaption{Third customer.}
\label{fig:decompositiondaycustomer3}
\end{minipage}
\label{fig:decompositiondaycustomer}
\caption{The time series decompositions of the daily consumption of the three customers considering as period of the time series a week.}
\end{figure}

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/daily_aggregated_correlation_customer1}
\subcaption{First customer.}
\label{fig:correlationdaycustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/daily_aggregated_correlation_customer2}
\subcaption{Second customer.}
\label{fig:correlationdaycustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.4\textwidth]{images/baseline/daily_aggregated_correlation_customer3}
\subcaption{Third customer.}
\label{fig:correlationdaycustomer3}
\end{minipage}
\caption{The auto-correlations of the daily consumption of the three customers.}
\end{figure}

\begin{figure}[H]
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/ft_day_customer1}
\subcaption{First customer.}
\label{fig:ftdaycustomer1}
\end{minipage}
\ \hspace{2mm} \
\begin{minipage}[b]{8.5cm}
\centering
\includegraphics[width=0.8\textwidth]{images/baseline/ft_day_customer2}
\subcaption{Second customer.}
\label{fig:ftdaycustomer2}
\end{minipage}
\begin{minipage}[b]{17cm}
\centering
\includegraphics[width=0.4\textwidth]{images/baseline/ft_day_customer3}
\subcaption{Third customer.}
\label{fig:ftdaycustomer3}
\end{minipage}
\caption{The coefficients given by the Fourier transform of the daily consumption of the three customers.}
\end{figure}

The results showed that the daily consumption of the three customers had:
\begin{itemize}
  \item a Pearson correlation coefficient of -0.5542, -0.2341, and -0.6727 respectively and a Spearman's rank correlation coefficient of -0.5164, -0.2592, and -0.6978 respectively with respect to the air temperature;
  \item a Pearson correlation coefficient of -0.5361, -0.2329, and -0.6787 respectively and a Spearman's rank correlation coefficient of -0.5166, -0.2584, and -0.7004 respectively with respect to the apparent temperature;
  \item a Pearson correlation coefficient of 0.1264, 0.0199, and 0.1407 respectively and a Spearman's rank correlation coefficient of 0.1412, 0.0506, and 0.2104 respectively with respect to the relative humidity.
\end{itemize}
It can be noticed that both the coefficients increased considering the daily data meaning that the mean weather data over the day is more correlated to the daily consumption of the three considered customers, compared to the hourly granularity.

% TODO What about tariff aggregation? probably not to include (maybe just some words)

After this data analysis, the specific parameters used in the models can be explained more in detail.
The baseline approaches are built considering the repetition of past days and weeks since data presents a high correlation with that time instants and could lead to a reasonable baseline performance to achieve.
The SARIMA model considers the week as the period for seasonal differencing since data presents a high correlation with that time instants and the model can try to take advantage of the weekly seasonality.
The support vector regressor model uses a radial basis function kernel with a configuration of the C parameter (regularization parameter of squared l2 penalty) to 1.0 to penalize the complexity of the model, and the epsilon parameter defining the epsilon-tube for no penalty to 0.1 as a trade-off between accuracy and generalization.
The hist gradient boosting regressor model uses as loss the absolute error since this was the supported metric closer to the objective and keeping the default parameters for tree definition.
The same was also done for the extreme gradient boosting regressor model.
The Prophet model was built keeping the default parameters for automatically detecting seasonalities and best fitting the training data.

The LSTM model was designed as a 2-layer model with 32 Bidirectional LSTM units in the first layer which use ReLU activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
The second layer composed of 16 Bidirectional LSTM units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using a combination of mean absolute error and mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The GRU model was designed as a 2-layer model with 32 Bidirectional GRU units in the first layer which use ReLU activation function and sigmoid as recurrent activation function with both dropout and recurrent dropout of 0.02.
Subsequently, the second layer is composed of 16 Bidirectional GRU units which use ReLU activation function and sigmoid as recurrent activation function without dropout and recurrent dropout.
The output of the layer goes inside a dense unit to output the final prediction.
The model is trained using a combination of mean absolute error and mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The CNN model was designed as a 2-layer model with a first layer composed of 32 1D Convolutional units with a kernel size of 5 which uses ReLU activation function.
A 1D max polling operation is applied before entering the second layer composed of 16 1D Convolutional units with a kernel size of 3 which use ReLU activation function.
A 1D max polling operation is applied before the flatting operation and entering the final dense unit to output the final prediction.
The model is trained using a combination of mean absolute error and mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The TFT model was configured with 2 LSTM layers with 16 as hidden size, 4 as attention size, and 8 as hidden continuous size.
The dropout is set to 0.02.
The model is trained using mean absolute percentage error as loss and Nadam as optimizer with a learning rate of 0.005.

The AutoML approach uses the mean MAPE forecasting as loss and it is launched for 10 hours with a maximum function evaluation time of 2 hours.
Then the best ensemble of the found models is returned and can be used to forecast.

12 splits were used for block validation with a test size of a week each time, namely 7 days of data.
For the hourly granularity, the models start with the total number of entries for the customer minus 2016 entries as training and predict every time 168 entries until reaching the last prediction instant where the model has a training size of the total number of entries for the customer minus the last 168 entries.
The results for hourly aggregation for the third customer are reported in table~\ref{tab:baselinehourlyresults}.

% Not done at forecasting time: not good results in training, spiegare perchè non sono buoni ... Mettere test set (last split) per far capire andamento

% first customer
% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
%  & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
% \textbf{Model} & \textbf{cross-validation} & \textbf{MAPE | MAE [kWh]}\\
%  & \textbf{MAPE | MAE [kWh]} & \\
% \hline
% HistGradientBoostingRegressor & 62.05 $\pm$ 15.29 | 0.132 $\pm$ 0.046 & 27.60 | 0.021\\
% \hline
% One Day Baseline & 171.37 $\pm$ 148.43 | 0.209 $\pm$ 0.083 & 48.44 | 0.028\\
% \hline
% GRU & 54.80 $\pm$ 8.11 | 0.235 $\pm$ 0.067 & 33.50 | 0.049\\
% \hline
% LSTM & 133.17 $\pm$ 207.08 | 0.294 $\pm$ 0.145 & 33.69 | 0.049\\
% \hline
% TFT & 74.25 $\pm$ 34.86 | 0.219 $\pm$ 0.066 & 39.10 | 0.055\\
% \hline
% XGBRegressor & 117.86 $\pm$ 31.29 | 0.157 $\pm$ 0.038 & 120.87 | 0.067\\
% \hline
% CNN & 167.29 $\pm$ 49.74 | 0.303 $\pm$ 0.067 & 165.55 | 0.110\\
% \hline
% SVR & 108.71 $\pm$ 33.24 | 0.225 $\pm$ 0.056 & 201.16 | 0.121\\
% \hline
% One Week Baseline & 178.58 $\pm$ 76.74 | 0.226 $\pm$ 0.039 & 322.74 | 0.166\\
% \hline
% 4 Week Baseline & 196.68 $\pm$ 78.00 | 0.228 $\pm$ 0.034 & 379.96 | 0.194\\
% \hline
% 12 Week Baseline & 225.54 $\pm$ 86.94 | 0.260 $\pm$ 0.029 & 453.56 | 0.231\\
% \hline
% SARIMA & 278.71 $\pm$ 63.55 | 0.325 $\pm$ 0.059 & 513.50 | 0.252\\
% \hline
% Prophet & 215.51 $\pm$ 113.13 | 0.253 $\pm$ 0.025 & 487.29 | 0.258\\
% \hline
% \end{tabular}
% \caption{Table summarizing the results for hourly aggregation.}
% \label{tab:baselinehourlyresults}
% \end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
\textbf{Model} & \textbf{cross-validation} & \textbf{MAPE | MAE [kWh]}\\
 & \textbf{MAPE | MAE [kWh]} & \\
\hline
HistGradientBoostingRegressor & 62.36 $\pm$ 26.63 | 0.142 $\pm$ 0.071 & 54.29 | 0.119\\
\hline
SVR & 146.78 $\pm$ 53.02 | 0.205 $\pm$ 0.043 & 92.01 | 0.160\\
\hline
XGBRegressor & 121.36 $\pm$ 34.22 | 0.184 $\pm$ 0.068 & 103.16 | 0.161\\
\hline
12 Week Baseline & 214.61 $\pm$ 94.84 | 0.256 $\pm$ 0.046 & 122.51 | 0.175\\
\hline
SARIMA & 170.81 $\pm$ 67.39 | 0.223 $\pm$ 0.051 & 124.07 | 0.176\\
\hline
4 Week Baseline & 151.74 $\pm$ 67.48 | 0.218 $\pm$ 0.046 & 110.45 | 0.180\\
\hline
One Week Baseline & 142.69 $\pm$ 69.22 | 0.232 $\pm$ 0.094 & 102.25 | 0.190\\
\hline
Prophet & 200.64 $\pm$ 59.18 | 0.255 $\pm$ 0.031 & 147.07 | 0.208\\
\hline
LSTM & 101.84 $\pm$ 4.67 | 0.246 $\pm$ 0.095 & 97.98 | 0.233\\  % TODO relaunch
\hline
GRU & 99.92 $\pm$ 5.13 | 0.243 $\pm$ 0.093 & 99.31 | 0.235\\  % TODO relaunch
\hline
TFT & 100.22 $\pm$ 44.40 | 0.242 $\pm$ 0.109 & 105.18 | 0.243\\
\hline
One Day Baseline & 151.22 $\pm$ 89.08 | 0.242 $\pm$ 0.101 & 162.76 | 0.247\\
\hline
CNN & 1749.85 $\pm$ 2806.48 | 2.035 $\pm$ 3.380 & 546.16 | 0.665\\  % TODO relaunch
\hline
\end{tabular}
\caption{Table summarizing the results for hourly aggregation.}
\label{tab:baselinehourlyresults}
\end{table}

Instead, the AutoML approach obtained a one-week MAPE of 61.64 and a MAE of 0.338 [kWh] after a total run of 10 hours.

Analyze the forecasts of the models for the electricity demand forecasting task (predictive analytics) ...

For the daily granularity, the models start with the total number of entries for the customer minus 84 entries as training and predict every time 30 entries until reaching the last prediction instant where the model has a training size of total number of entries for the customer minus the last 7 entries.
The results for daily aggregation for the third customer are reported in table~\ref{tab:baselinedailyresults}.

% first customer
% \begin{table}[H]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
%  & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
% \textbf{Model} & \textbf{cross-validation} & \textbf{MAPE | MAE [kWh]}\\
%  & \textbf{MAPE | MAE [kWh]} & \\
% \hline
% One Day Baseline & 63.23 $\pm$ 84.77 | 2.917 $\pm$ 2.111 & 5.86 | 0.140\\
% \hline
% TFT & 57.49 $\pm$ 53.43 | 2.576 $\pm$ 1.169 & 48.81 | 1.010\\
% \hline
% CNN & 65.10 $\pm$ 37.05 | 4.226 $\pm$ 1.829 & 50.93 | 1.092\\
% \hline
% GRU & 46.34 $\pm$ 16.29 | 3.409 $\pm$ 1.652 & 61.09 | 1.298\\
% \hline
% XGBRegressor & 50.01 $\pm$ 31.96 | 2.597 $\pm$ 0.890 & 89.73 | 1.939\\
% \hline
% HistGradientBoostingRegressor & 45.34 $\pm$ 28.64 | 2.362 $\pm$ 0.682 & 102.34 | 2.115\\
% \hline
% LSTM & 59.81 $\pm$ 20.88 | 3.813 $\pm$ 1.425 & 119.40 | 2.522\\
% \hline
% Prophet & 56.81 $\pm$ 39.79 | 3.058 $\pm$ 1.064 & 159.90 | 3.451\\
% \hline
% One Week Baseline & 66.31 $\pm$ 50.24 | 3.036 $\pm$ 1.057 & 166.09 | 3.461\\
% \hline
% SVR & 57.95 $\pm$ 53.96 | 2.677 $\pm$ 1.056 & 206.13 | 4.366\\
% \hline
% 4 Week Baseline & 65.55 $\pm$ 58.37 | 2.830 $\pm$ 0.991 & 215.05 | 4.491\\
% \hline
% SARIMA & 59.67 $\pm$ 46.79 | 2.668 $\pm$ 1.087 & 225.34 | 4.790\\
% \hline
% 12 Week Baseline & 64.66 $\pm$ 61.13 | 2.832 $\pm$ 1.158 & 233.05 | 4.945\\
% \hline
% \end{tabular}
% \caption{Table summarizing the results for daily aggregation.}
% \label{tab:baselinedailyresults}
% \end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Blocked k-fold} & \textbf{Test on the last split}\\
\textbf{Model} & \textbf{cross-validation} & \textbf{MAPE | MAE [kWh]}\\
 & \textbf{MAPE | MAE [kWh]} & \\
\hline
SARIMA & 58.89 $\pm$ 63.37 | 2.195 $\pm$ 1.161 & 50.13 | 1.683\\
\hline
12 Week Baseline & 103.09 $\pm$ 115.41 | 3.223 $\pm$ 1.725 & 50.29 | 1.700\\
\hline
One Day Baseline & 58.01 $\pm$ 74.41 | 2.635 $\pm$ 2.050 & 58.21 | 1.874\\
\hline
One Week Baseline & 46.43 $\pm$ 51.52 | 2.115 $\pm$ 1.134 & 43.71 | 1.877\\
\hline
4 Week Baseline & 69.42 $\pm$ 73.32 | 2.491 $\pm$ 1.287 & 51.33 | 1.939\\
\hline
HistGradientBoostingRegressor & 50.34 $\pm$ 61.31 | 2.036 $\pm$ 1.087 & 57.32 | 2.252\\
\hline
LSTM & 139.76 $\pm$ 226.50 | 4.092 $\pm$ 4.066 & 44.51 | 2.347\\
\hline
Prophet & 62.68 $\pm$ 57.72 | 2.516 $\pm$ 1.095 & 42.52 | 2.497\\
\hline
TFT & 55.70 $\pm$ 46.32 | 2.508 $\pm$ 1.143 & 44.25 | 2.587\\
\hline
XGBRegressor & 50.11 $\pm$ 56.39 | 2.027 $\pm$ 1.003 & 75.51 | 3.045\\
\hline
GRU & 60.44 $\pm$ 53.78 | 2.513 $\pm$ 1.044 & 62.95 | 3.940\\
\hline
CNN & 85.10 $\pm$ 97.77 | 3.349 $\pm$ 1.817 & 104.54 | 5.931\\
\hline
SVR & 199.33 $\pm$ 179.16 | 6.763 $\pm$ 2.162 & 158.47 | 6.146\\
\hline
\end{tabular}
\caption{Table summarizing the results for daily aggregation.}
\label{tab:baselinedailyresults}
\end{table}

Instead, the AutoML approach obtained a one-week MAPE of 19.88 and a MAE of 2.197 [kWh] after a total run of 10 hours.

% TODO check the consistency of model results in the last period and using cross-validation --> do a table with the differences (+ plot for internal analysis)
% commentarli (sia il numero che il grafico dell’errore) --> interpretare il dato (perchè è buono?) --> benchmark in letteratura, ecc…
% [valutare criticamente, aspetto tecnologico scientifico]
